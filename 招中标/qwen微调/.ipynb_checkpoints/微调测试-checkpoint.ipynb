{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T01:54:32.216400Z",
     "start_time": "2024-11-17T01:54:32.078970Z"
    }
   },
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from fontTools.ttLib.tables.ttProgram import instructions\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq, GenerationConfig\n",
    "\n",
    "# from policy_crawling.datacleaning.test import tokenizer"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "a9489aac0f727c23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T01:54:35.391736Z",
     "start_time": "2024-11-17T01:54:34.637341Z"
    }
   },
   "source": [
    "# 将Json文件转换为CSV文件\n",
    "df = pd.read_json('微调数据.json')\n",
    "ds = Dataset.from_pandas(df)\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "c735e78e1ab9cf11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T01:54:35.746336Z",
     "start_time": "2024-11-17T01:54:35.727339Z"
    }
   },
   "source": "ds[:3]",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': ['你是一名实体提取、意图识别和分类领域专家，请严格遵循以下任务和工作流程的指示输出结果。\\n\\n任务：\\n1-分析用户输入问题的语义信息和语法结构。\\n2-根据分析结果，将问题中的停用词、无意图词、语气词和符号剔除。\\n3-检查是否存在剔除遗漏，是否存在错误剔除，并进行矫正。\\n4-将保留的文本进行实体、关键词、意图词、主语词、设备词识别。\\n5-识别出的词以逗号分隔。\\n6-根据分析结果、实体提取结果和给出的意图分类标签进行问题的意图分类。\\n7-根据输出格式输出。\\n\\n意图标签：商品数据，招标中标信息，政策内容，政策文件信息，日常问题。\\n\\n请根据分类结果与要求的输出格式对比，存在格式问题则进行矫正后输出。\\n\\n输出格式：\\n实体识别结果：实体，实体\\n意图分类：意图标签\\n\\n请注意输出结果严格按照输出格式进行，不需要输出分析过程。不要输出实体结果。\\n\\n问题：医用DR设备都有哪些品牌？',\n",
       "  '你是一名实体提取、意图识别和分类领域专家，请严格遵循以下任务和工作流程的指示输出结果。\\n\\n任务：\\n1-分析用户输入问题的语义信息和语法结构。\\n2-根据分析结果，将问题中的停用词、无意图词、语气词和符号剔除。\\n3-检查是否存在剔除遗漏，是否存在错误剔除，并进行矫正。\\n4-将保留的文本进行实体、关键词、意图词、主语词、设备词识别。\\n5-识别出的词以逗号分隔。\\n6-根据分析结果、实体提取结果和给出的意图分类标签进行问题的意图分类。\\n7-根据输出格式输出。\\n\\n意图标签：商品数据，招标中标信息，政策内容，政策文件信息，日常问题。\\n\\n请根据分类结果与要求的输出格式对比，存在格式问题则进行矫正后输出。\\n\\n输出格式：\\n实体识别结果：实体，实体\\n意图分类：意图标签\\n\\n请注意输出结果严格按照输出格式进行，不需要输出分析过程。不要输出实体结果。\\n\\n问题：动态三维脊柱姿态测量分析装置北京的供应商有哪些品牌？',\n",
       "  '你是一名实体提取、意图识别和分类领域专家，请严格遵循以下任务和工作流程的指示输出结果。\\n\\n任务：\\n1-分析用户输入问题的语义信息和语法结构。\\n2-根据分析结果，将问题中的停用词、无意图词、语气词和符号剔除。\\n3-检查是否存在剔除遗漏，是否存在错误剔除，并进行矫正。\\n4-将保留的文本进行实体、关键词、意图词、主语词、设备词识别。\\n5-识别出的词以逗号分隔。\\n6-根据分析结果、实体提取结果和给出的意图分类标签进行问题的意图分类。\\n7-根据输出格式输出。\\n\\n意图标签：商品数据，招标中标信息，政策内容，政策文件信息，日常问题。\\n\\n请根据分类结果与要求的输出格式对比，存在格式问题则进行矫正后输出。\\n\\n输出格式：\\n实体识别结果：实体，实体\\n意图分类：意图标签\\n\\n请注意输出结果严格按照输出格式进行，不需要输出分析过程。不要输出实体结果。\\n\\n问题：医院住院大楼生活热水系统改造有哪些品牌？'],\n",
       " 'input': ['', '', ''],\n",
       " 'output': ['实体识别结果：医用DR设备，品牌\\n意图分类：商品数据',\n",
       "  '实体识别结果：动态三维脊柱姿态测量分析装置，供应商，品牌\\n意图分类：商品数据',\n",
       "  '实体识别结果：医院住院大楼生活热水系统改造，品牌\\n意图分类：商品数据']}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "d4736dcdfacd5d03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T01:54:30.077902300Z",
     "start_time": "2024-11-16T13:28:07.728827Z"
    }
   },
   "source": [
    "# 下载qwen2.5-3b模型\n",
    "# from modelscope import snapshot_download\n",
    "# model_dir = snapshot_download(\"Qwen/Qwen2.5-3B-Instruct\", revision=\"master\", cache_dir='./models')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 21:28:08,289 - modelscope - INFO - PyTorch version 2.2.2+cu118 Found.\n",
      "2024-11-16 21:28:08,295 - modelscope - INFO - Loading ast index from C:\\Users\\admin\\.cache\\modelscope\\ast_indexer\n",
      "2024-11-16 21:28:08,542 - modelscope - INFO - Loading done! Current index file version is 1.9.5, with md5 8efaaefcd5c439a25cc9085b5b0b2f0a and a total number of 945 components indexed\n",
      "2024-11-16 21:28:08,560 - modelscope - WARNING - Authentication has expired, please re-login if you need to access private models or datasets.\n",
      "Downloading: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 661/661 [00:00<00:00, 167kB/s]\n",
      "Downloading: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2.00/2.00 [00:00<00:00, 666B/s]\n",
      "Downloading: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 242/242 [00:00<00:00, 60.5kB/s]\n",
      "Downloading: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7.21k/7.21k [00:00<00:00, 2.46MB/s]\n",
      "Downloading: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.59M/1.59M [00:00<00:00, 4.89MB/s]\n",
      "Downloading: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉| 3.70G/3.70G [03:35<00:00, 18.4MB/s]\n",
      "Downloading:   0%|                                                                                                                                     | 0.00/2.05G [00:00<?, ?B/s]2024-11-16 21:32:48,287 - modelscope - WARNING - Download file from: 503316480 to: 671088639 failed, will retry\n",
      "Downloading: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉| 2.05G/2.05G [02:36<00:00, 14.1MB/s]\n",
      "Downloading: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 34.7k/34.7k [00:00<00:00, 1.02MB/s]\n",
      "Downloading: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4.79k/4.79k [00:00<00:00, 700kB/s]\n",
      "Downloading: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6.71M/6.71M [00:00<00:00, 11.6MB/s]\n",
      "Downloading: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7.13k/7.13k [00:00<00:00, 1.83MB/s]\n",
      "Downloading: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2.65M/2.65M [00:00<00:00, 6.49MB/s]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 加载tokenizer",
   "id": "841f174fc71ebfcd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T01:55:35.566881Z",
     "start_time": "2024-11-17T01:55:35.320883Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 加载分词器\n",
    "model_dir = 'C:/Users/admin/Desktop/kedaxunfei/qwen2.5-3b数据集构建/models/Qwen/Qwen2.5-3B-Instruct'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=False, trust_remote_code=True)\n",
    "tokenizer"
   ],
   "id": "e4223433db99f468",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen2Tokenizer(name_or_path='C:/Users/admin/Desktop/kedaxunfei/qwen2.5-3b数据集构建/models/Qwen/Qwen2.5-3B-Instruct', vocab_size=151643, model_max_length=131072, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T01:55:37.605930Z",
     "start_time": "2024-11-17T01:55:37.592881Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 构建数据转编码函数\n",
    "def process_func(example):\n",
    "    MAX_LENGTH = 1800\n",
    "    instruction = tokenizer(f\"<|im_start|>system\\n你是一名实体提取、意图识别和分类领域专家，请严格遵循以下任务和工作流程的指示输出结果。<|im_end|>\\n<|im_start|>user\\n{example['instruction']+example['input']}<|im_end|>\\n<|im_start|>assistant\\n\", add_special_tokens=False)\n",
    "    response = tokenizer(f\"{example['output']}\", add_special_tokens=False)\n",
    "    input_ids = instruction['input_ids'] + response['input_ids'] + [tokenizer.pad_token_id]\n",
    "    attention_mask = instruction['attention_mask'] + response['attention_mask'] + [1]\n",
    "    labels = [-100] * len(instruction['input_ids']) + response['input_ids'] + [tokenizer.pad_token_id]\n",
    "\n",
    "    if len(input_ids) > MAX_LENGTH:\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        attention_mask = attention_mask[:MAX_LENGTH]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': labels\n",
    "    }\n"
   ],
   "id": "4dab48eac8a609d8",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 数据格式化",
   "id": "e7660f255ecfaef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T01:55:45.506937Z",
     "start_time": "2024-11-17T01:55:38.612881Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_id = ds.map(process_func, remove_columns=ds.column_names)\n",
    "tokenized_id"
   ],
   "id": "1aa6a92b688f3510",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/437 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c6cdaca626d344d0ab1c52403276acd1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 437\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T01:59:03.199697Z",
     "start_time": "2024-11-17T01:59:03.082664Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.decode(tokenized_id[0]['input_ids'])",
   "id": "558184b212a19a78",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\n你是一名实体提取、意图识别和分类领域专家，请严格遵循以下任务和工作流程的指示输出结果。<|im_end|>\\n<|im_start|>user\\n你是一名实体提取、意图识别和分类领域专家，请严格遵循以下任务和工作流程的指示输出结果。\\n\\n任务：\\n1-分析用户输入问题的语义信息和语法结构。\\n2-根据分析结果，将问题中的停用词、无意图词、语气词和符号剔除。\\n3-检查是否存在剔除遗漏，是否存在错误剔除，并进行矫正。\\n4-将保留的文本进行实体、关键词、意图词、主语词、设备词识别。\\n5-识别出的词以逗号分隔。\\n6-根据分析结果、实体提取结果和给出的意图分类标签进行问题的意图分类。\\n7-根据输出格式输出。\\n\\n意图标签：商品数据，招标中标信息，政策内容，政策文件信息，日常问题。\\n\\n请根据分类结果与要求的输出格式对比，存在格式问题则进行矫正后输出。\\n\\n输出格式：\\n实体识别结果：实体，实体\\n意图分类：意图标签\\n\\n请注意输出结果严格按照输出格式进行，不需要输出分析过程。不要输出实体结果。\\n\\n问题：医用DR设备都有哪些品牌？<|im_end|>\\n<|im_start|>assistant\\n实体识别结果：医用DR设备，品牌\\n意图分类：商品数据<|endoftext|>'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T02:01:29.826070Z",
     "start_time": "2024-11-17T02:01:29.818038Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 需要过滤掉-100的位置既是输入的文本信息\n",
    "tokenizer.decode(list(filter(lambda x:x!=-100, tokenized_id[0]['labels'])))"
   ],
   "id": "b1ed37edfb7a37f2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'实体识别结果：医用DR设备，品牌\\n意图分类：商品数据<|endoftext|>'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 加载半精度模型",
   "id": "95b8f5e33fdf67f7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T03:22:44.974149Z",
     "start_time": "2024-11-17T03:22:29.984148Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "model = AutoModelForCausalLM.from_pretrained(model_dir,\n",
    "                                             trust_remote_code=True,\n",
    "                                             # device_map='auto',\n",
    "                                             torch_dtype=torch.bfloat16)\n",
    "model"
   ],
   "id": "25c39af9d012ff68",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "729cdb42a5024204897fbc50671193e5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-35): 36 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2SdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm()\n",
       "        (post_attention_layernorm): Qwen2RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T03:22:52.557149Z",
     "start_time": "2024-11-17T03:22:52.546146Z"
    }
   },
   "cell_type": "code",
   "source": "model.enable_input_require_grads()  # 开启梯度检查点时，要执行该方法",
   "id": "53918f1e0ff6007b",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T03:22:53.568192Z",
     "start_time": "2024-11-17T03:22:53.563148Z"
    }
   },
   "cell_type": "code",
   "source": "model.dtype",
   "id": "b509227f1413767b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.bfloat16"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 定义LoraConfig",
   "id": "6312b81ac75bc88f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T03:22:54.772168Z",
     "start_time": "2024-11-17T03:22:54.764147Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    # 需要训练的模型层的名字，主要就是attention部分的层，\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    inference_mode=False,   # 训练模式\n",
    "    r=8,  # Lora 秩\n",
    "    lora_alpha=32,  # Lora alpha 控制了 LoRA 变换的缩放因子\n",
    "    lora_dropout=0.1,  # Lora dropout\n",
    ")\n",
    "config"
   ],
   "id": "a7cd18b62471bc47",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, inference_mode=False, r=8, target_modules={'q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj', 'k_proj', 'o_proj'}, lora_alpha=32, lora_dropout=0.1, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T03:22:56.717145Z",
     "start_time": "2024-11-17T03:22:56.138187Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = get_peft_model(model, config)\n",
    "config"
   ],
   "id": "f8ccff3d61ff5cd9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='C:/Users/admin/Desktop/kedaxunfei/qwen2.5-3b数据集构建/models/Qwen/Qwen2.5-3B-Instruct', revision=None, task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, inference_mode=False, r=8, target_modules={'q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj', 'k_proj', 'o_proj'}, lora_alpha=32, lora_dropout=0.1, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T03:22:57.934147Z",
     "start_time": "2024-11-17T03:22:57.923146Z"
    }
   },
   "cell_type": "code",
   "source": "model.print_trainable_parameters()",
   "id": "7ea3b11ddd474bb8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 14,966,784 || all params: 3,100,905,472 || trainable%: 0.4827\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 配置训练参数",
   "id": "b9b6484b8f14911a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T03:22:58.963549Z",
     "start_time": "2024-11-17T03:22:58.826150Z"
    }
   },
   "cell_type": "code",
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir='./output/Qwen3b_instruct_lora',\n",
    "    # batch_size\n",
    "    per_device_train_batch_size=1,\n",
    "    # 梯度累加，如果你的显存比较小，那可以把 batch_size 设置小一点，梯度累加增大一些\n",
    "    gradient_accumulation_steps=4,\n",
    "    # 多少步，输出一次log\n",
    "    logging_steps=1,\n",
    "    # 顾名思义 epoch\n",
    "    num_train_epochs=1,\n",
    "    save_steps=10,\n",
    "    learning_rate=1e-4,\n",
    "    save_on_each_node=True,\n",
    "    # 梯度检查，这个一旦开启，模型就必须执行model.enable_input_require_grads()\n",
    "    gradient_checkpointing=True\n",
    ")"
   ],
   "id": "de2dc667e9e7502b",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T03:23:04.251155Z",
     "start_time": "2024-11-17T03:23:00.442146Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_id,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True)\n",
    ")"
   ],
   "id": "81d5a4d4b543337b",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T03:33:58.774140Z",
     "start_time": "2024-11-17T03:33:58.769136Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 禁用内存高效的张量收缩算法,这个函数通常用于改善大规模矩阵乘法操作的内存使用效率，特别是在使用 GPU 时。\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n"
   ],
   "id": "497fa967ce985000",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T04:50:42.933700Z",
     "start_time": "2024-11-17T03:49:03.968759Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 开始训练\n",
    "trainer.train()"
   ],
   "id": "76df7dd4573f4f0b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "F:\\Anaconda3\\envs\\qwen\\lib\\site-packages\\torch\\utils\\checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "F:\\Anaconda3\\envs\\qwen\\lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:688: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11' max='109' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 11/109 53:24 < 9:41:31, 0.00 it/s, Epoch 0.09/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "\n            Some tensors share memory, this will lead to duplicate memory on disk and potential differences when loading them again: [{'base_model.model.model.embed_tokens.weight', 'base_model.model.lm_head.weight'}].\n            A potential way to correctly save your model is to use `save_model`.\n            More information at https://huggingface.co/docs/safetensors/torch_shared_tensors\n            ",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[34], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# 开始训练\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mF:\\Anaconda3\\envs\\qwen\\lib\\site-packages\\transformers\\trainer.py:1932\u001B[0m, in \u001B[0;36mTrainer.train\u001B[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[0;32m   1930\u001B[0m         hf_hub_utils\u001B[38;5;241m.\u001B[39menable_progress_bars()\n\u001B[0;32m   1931\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1932\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1933\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1934\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1935\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1936\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1937\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mF:\\Anaconda3\\envs\\qwen\\lib\\site-packages\\transformers\\trainer.py:2345\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[0;32m   2342\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mepoch \u001B[38;5;241m=\u001B[39m epoch \u001B[38;5;241m+\u001B[39m (step \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m steps_skipped) \u001B[38;5;241m/\u001B[39m steps_in_epoch\n\u001B[0;32m   2343\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_handler\u001B[38;5;241m.\u001B[39mon_step_end(args, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol)\n\u001B[1;32m-> 2345\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_maybe_log_save_evaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtr_loss\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_norm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepoch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2346\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   2347\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_handler\u001B[38;5;241m.\u001B[39mon_substep_end(args, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol)\n",
      "File \u001B[1;32mF:\\Anaconda3\\envs\\qwen\\lib\\site-packages\\transformers\\trainer.py:2796\u001B[0m, in \u001B[0;36mTrainer._maybe_log_save_evaluate\u001B[1;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001B[0m\n\u001B[0;32m   2793\u001B[0m     metrics \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_evaluate(trial, ignore_keys_for_eval)\n\u001B[0;32m   2795\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol\u001B[38;5;241m.\u001B[39mshould_save:\n\u001B[1;32m-> 2796\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_save_checkpoint\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetrics\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetrics\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2797\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_handler\u001B[38;5;241m.\u001B[39mon_save(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol)\n",
      "File \u001B[1;32mF:\\Anaconda3\\envs\\qwen\\lib\\site-packages\\transformers\\trainer.py:2875\u001B[0m, in \u001B[0;36mTrainer._save_checkpoint\u001B[1;34m(self, model, trial, metrics)\u001B[0m\n\u001B[0;32m   2873\u001B[0m run_dir \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_output_dir(trial\u001B[38;5;241m=\u001B[39mtrial)\n\u001B[0;32m   2874\u001B[0m output_dir \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(run_dir, checkpoint_folder)\n\u001B[1;32m-> 2875\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutput_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_internal_call\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m   2877\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39msave_only_model:\n\u001B[0;32m   2878\u001B[0m     \u001B[38;5;66;03m# Save optimizer and scheduler\u001B[39;00m\n\u001B[0;32m   2879\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_save_optimizer_and_scheduler(output_dir)\n",
      "File \u001B[1;32mF:\\Anaconda3\\envs\\qwen\\lib\\site-packages\\transformers\\trainer.py:3429\u001B[0m, in \u001B[0;36mTrainer.save_model\u001B[1;34m(self, output_dir, _internal_call)\u001B[0m\n\u001B[0;32m   3426\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_wrapped\u001B[38;5;241m.\u001B[39msave_checkpoint(output_dir)\n\u001B[0;32m   3428\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mshould_save:\n\u001B[1;32m-> 3429\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_save\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutput_dir\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3431\u001B[0m \u001B[38;5;66;03m# Push to the Hub when `save_model` is called by the user.\u001B[39;00m\n\u001B[0;32m   3432\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mpush_to_hub \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _internal_call:\n",
      "File \u001B[1;32mF:\\Anaconda3\\envs\\qwen\\lib\\site-packages\\transformers\\trainer.py:3494\u001B[0m, in \u001B[0;36mTrainer._save\u001B[1;34m(self, output_dir, state_dict)\u001B[0m\n\u001B[0;32m   3492\u001B[0m logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTrainer.model is not a `PreTrainedModel`, only saving its state dict.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   3493\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39msave_safetensors:\n\u001B[1;32m-> 3494\u001B[0m     \u001B[43msafetensors\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave_file\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   3495\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstate_dict\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutput_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mSAFE_WEIGHTS_NAME\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mformat\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpt\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m}\u001B[49m\n\u001B[0;32m   3496\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3497\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   3498\u001B[0m     torch\u001B[38;5;241m.\u001B[39msave(state_dict, os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(output_dir, WEIGHTS_NAME))\n",
      "File \u001B[1;32mF:\\Anaconda3\\envs\\qwen\\lib\\site-packages\\safetensors\\torch.py:286\u001B[0m, in \u001B[0;36msave_file\u001B[1;34m(tensors, filename, metadata)\u001B[0m\n\u001B[0;32m    255\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msave_file\u001B[39m(\n\u001B[0;32m    256\u001B[0m     tensors: Dict[\u001B[38;5;28mstr\u001B[39m, torch\u001B[38;5;241m.\u001B[39mTensor],\n\u001B[0;32m    257\u001B[0m     filename: Union[\u001B[38;5;28mstr\u001B[39m, os\u001B[38;5;241m.\u001B[39mPathLike],\n\u001B[0;32m    258\u001B[0m     metadata: Optional[Dict[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mstr\u001B[39m]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    259\u001B[0m ):\n\u001B[0;32m    260\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    261\u001B[0m \u001B[38;5;124;03m    Saves a dictionary of tensors into raw bytes in safetensors format.\u001B[39;00m\n\u001B[0;32m    262\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    284\u001B[0m \u001B[38;5;124;03m    ```\u001B[39;00m\n\u001B[0;32m    285\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 286\u001B[0m     serialize_file(\u001B[43m_flatten\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m)\u001B[49m, filename, metadata\u001B[38;5;241m=\u001B[39mmetadata)\n",
      "File \u001B[1;32mF:\\Anaconda3\\envs\\qwen\\lib\\site-packages\\safetensors\\torch.py:488\u001B[0m, in \u001B[0;36m_flatten\u001B[1;34m(tensors)\u001B[0m\n\u001B[0;32m    485\u001B[0m         failing\u001B[38;5;241m.\u001B[39mappend(names)\n\u001B[0;32m    487\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m failing:\n\u001B[1;32m--> 488\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m    489\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[0;32m    490\u001B[0m \u001B[38;5;124m        Some tensors share memory, this will lead to duplicate memory on disk and potential differences when loading them again: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfailing\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\n\u001B[0;32m    491\u001B[0m \u001B[38;5;124m        A potential way to correctly save your model is to use `save_model`.\u001B[39m\n\u001B[0;32m    492\u001B[0m \u001B[38;5;124m        More information at https://huggingface.co/docs/safetensors/torch_shared_tensors\u001B[39m\n\u001B[0;32m    493\u001B[0m \u001B[38;5;124m        \u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[0;32m    494\u001B[0m     )\n\u001B[0;32m    496\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m {\n\u001B[0;32m    497\u001B[0m     k: {\n\u001B[0;32m    498\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdtype\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mstr\u001B[39m(v\u001B[38;5;241m.\u001B[39mdtype)\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m)[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    502\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m tensors\u001B[38;5;241m.\u001B[39mitems()\n\u001B[0;32m    503\u001B[0m }\n",
      "\u001B[1;31mRuntimeError\u001B[0m: \n            Some tensors share memory, this will lead to duplicate memory on disk and potential differences when loading them again: [{'base_model.model.model.embed_tokens.weight', 'base_model.model.lm_head.weight'}].\n            A potential way to correctly save your model is to use `save_model`.\n            More information at https://huggingface.co/docs/safetensors/torch_shared_tensors\n            "
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 加载 lora 权重推理\n",
    "## 合并加载模型"
   ],
   "id": "e36f505fd6e8b9b6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f05eba7fb018b3b6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1260a291d39d8735"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
