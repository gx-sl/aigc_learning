{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "906ce176-935f-44d0-b65f-480deacfed19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from typing import Optional, List, Any\n",
    "import torch\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig, TextIteratorStreamer, AutoProcessor\n",
    "# from modelscope import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.outputs.generation import GenerationChunk\n",
    "from torch import device\n",
    "import time\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain.chains.history_aware_retriever import create_history_aware_retriever\n",
    "\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain_core.runnables import RunnableWithMessageHistory\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from transformers import pipeline\n",
    "\n",
    "# from threading import Thread\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4304801-9b8d-4b5d-b120-9f276946133b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bb71bc6-feb2-4935-b026-95862673728f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 构建模型\n",
    "class GemmaLLM(LLM):\n",
    "    # 基于本地的QianWen7B-Chat模型自定义LLM类\n",
    "    tokenizer: AutoTokenizer = None\n",
    "    model: AutoModelForCausalLM = None\n",
    "    processor: AutoProcessor = None\n",
    "    \n",
    "    def __init__(self, model_dir: str):\n",
    "        # 从本地加载模型\n",
    "        super().__init__()\n",
    "        print('正从本地加载模型。。。。。')\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            pretrained_model_name_or_path=model_dir,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            pretrained_model_name_or_path=model_dir,\n",
    "            # device_map='auto',\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            # temperature=0.1\n",
    "            ).to(device)\n",
    "        self.model = self.model.eval()\n",
    "        self.model.generation_config = GenerationConfig.from_pretrained(\n",
    "            model_dir,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        # 可指定不同的生成长度、top_p等相关超参\n",
    "        self.processor = AutoProcessor.from_pretrained(model_dir)\n",
    "        print('模型加载完成！')\n",
    "\n",
    "    def _call(self, messages,\n",
    "              stop: Optional[List[str]] = None,\n",
    "              run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "              **kwargs: Any):\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": messages}\n",
    "        ]\n",
    "        text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        model_inputs = self.tokenizer([text], return_tensors='pt').to(device)\n",
    "        # print('input_ids:', input_ids)\n",
    "        # input_ = torch.tensor([input_ids])\n",
    "        # outputs = self.model.generate(**input_ids,max_new_tokens=1024)\n",
    "        # response = outputs[0][input_ids.shape[-1] :]\n",
    "        # res = self.tokenizer.decode(response, skip_special_tokens=True)\n",
    "        generated_ids = self.model.generate(\n",
    "            **model_inputs,\n",
    "            max_new_tokens=1024\n",
    "        )\n",
    "        generated_ids = [\n",
    "            output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "        response = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "        \n",
    "        # print('模型输出：',response)\n",
    "        return response\n",
    "        \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"Gemma2LM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a0aa4c1-af87-4d29-8478-9875ca3278a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_db():\n",
    "    persist_directory = r'vectordb/chroma/m3e_base'\n",
    "    embeddings_model_cache_path = r'autodl-tmp/embedding_model/m3e/AI-ModelScope/m3e-base'\n",
    "    # 加载词向量模型\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=embeddings_model_cache_path)\n",
    "    # 加载缓存知识库\n",
    "    vectordb = Chroma(\n",
    "        persist_directory=persist_directory,\n",
    "        embedding_function=embeddings,\n",
    "    )\n",
    "    return vectordb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ef66d60-7bcc-4ca2-866d-d1f6ecb4c9e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_model(vectordb):\n",
    "    # 初始化模型\n",
    "    # model_cache_path = r'autodl-tmp/Qwen/Qwen-7B-Chat'\n",
    "    model_cache_path = r'autodl-tmp/ZhipuAI/glm-4-9b-chat'\n",
    "    llm = GemmaLLM(model_dir=model_cache_path)\n",
    "    template = \"\"\"\n",
    "        使用上下文来回答最后的问题。如果你不知道答案，就说你不知道，不要试图编造答案。总是在回答的最后说“谢谢你的提问！”。\n",
    "        {context}\n",
    "        问题：{question}\n",
    "        \"\"\"\n",
    "    QA_CHAIN_PROMPT = PromptTemplate(input_variables=['context', 'question'], template=template)\n",
    "\n",
    "    # 构造检索问答链\n",
    "    qa_chain_ = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        retriever=vectordb.as_retriever(),\n",
    "        return_source_documents=True,\n",
    "        chain_type_kwargs={'prompt': QA_CHAIN_PROMPT},\n",
    "        # chain_type='stuff'\n",
    "    )\n",
    "    return qa_chain_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29745463-1518-40c4-b8da-274fe5bf6ce3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正从本地加载模型。。。。。\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baac78bab1e34feaaccdd46425d9c004",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "The repository for autodl-tmp/ZhipuAI/glm-4-9b-chat contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/autodl-tmp/ZhipuAI/glm-4-9b-chat.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n",
      "The repository for autodl-tmp/ZhipuAI/glm-4-9b-chat contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/autodl-tmp/ZhipuAI/glm-4-9b-chat.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型加载完成！\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 39/580 [05:00<1:16:40,  8.50s/it]"
     ]
    }
   ],
   "source": [
    "# 加载数据\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "# 加载数据文本，提取问题和真实数据源\n",
    "test_res = []\n",
    "qa_data = pd.read_excel('500条.xlsx').iloc[:, 2:]\n",
    "length = len(qa_data.iloc[0, :])\n",
    "history = []\n",
    "\n",
    "# 初始化模型\n",
    "vectordb = init_db()\n",
    "model = init_model(vectordb)\n",
    "\n",
    "file = 'glm4模型问答测试结果.csv'\n",
    "# data_res = pd.DataFrame([], columns=['Question','Answer', 'reference', 'Metadata', 'Text', '回答', 'RAG召回'])\n",
    "if os.path.exists(file):\n",
    "    data_tested = pd.read_csv(file, encoding='utf-8')['Question'].tolist()\n",
    "else:\n",
    "    data_tested = []\n",
    "for q in tqdm(range(len(qa_data))):\n",
    "    question = qa_data.iloc[q, 0]\n",
    "    \n",
    "    if question not in data_tested:\n",
    "        # print('打印问题:', question)\n",
    "        response = model.invoke({'query': question})#['result']\n",
    "        # print('打印问答链结果：', response)\n",
    "        answer = response['result']\n",
    "        call = response['source_documents']\n",
    "        # target = model._call(question)\n",
    "        # print('response参数：', response)\n",
    "        data_ = qa_data.iloc[q].values.tolist() + [response] + [call]\n",
    "        with open(file, 'a', newline='', encoding='utf-8') as f:\n",
    "            fw = csv.writer(f)\n",
    "            if not os.path.getsize(file):\n",
    "                header = ['Question', 'Answer', 'Metadata', 'Text','title', '回答', 'RAG召回']\n",
    "                fw.writerow(header)\n",
    "            fw.writerow(data_)\n",
    "    #         # 写入标题行\n",
    "    # writer.writerow(headers)\n",
    "    # data_.to_excel('qwen2.5模型测试结果.xlsx', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c2c8001-f20d-45fd-9c72-64d0173d1f10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(qa_data.iloc[q, :].values.tolist() + [response] + [call])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba99351-18de-4913-a087-3cdd21e1fedf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05487b3c-6b6d-4744-b89c-22239903b15f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9544957c-c9c7-4a2c-a1da-ae444c6594e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
