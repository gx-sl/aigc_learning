{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ad5bf67-3e97-4ad6-801a-ef63e5cd6770",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/root/miniconda3/lib/python3.8/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from typing import Optional, List, Any\n",
    "import torch\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig, TextIteratorStreamer, AutoProcessor\n",
    "# from modelscope import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.outputs.generation import GenerationChunk\n",
    "from torch import device\n",
    "import time\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain.chains.history_aware_retriever import create_history_aware_retriever\n",
    "\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain_core.runnables import RunnableWithMessageHistory\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "from threading import Thread\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763a10e4-9884-42a2-aade-0ebfe71c8ce8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 该部分代码实现\n",
    "## 1.历史记录聊天\n",
    "## 2.实体提取\n",
    "## 3.意图识别\n",
    "## 4.主体模型gemma-2-9B-it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d60fd1-f0c0-478f-a679-1cfd7d1e2349",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6cb65caa-1919-4599-bc5d-9023aec1081c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 构建模型\n",
    "class QianWenLLM(LLM):\n",
    "    # 基于本地的QianWen7B-Chat模型自定义LLM类\n",
    "    tokenizer: AutoTokenizer = None\n",
    "    model: AutoModelForCausalLM = None\n",
    "    processor: AutoProcessor = None\n",
    "    \n",
    "    def __init__(self, model_dir: str):\n",
    "        # 从本地加载模型\n",
    "        super().__init__()\n",
    "        print('正从本地加载模型。。。。。')\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            pretrained_model_name_or_path=model_dir,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            pretrained_model_name_or_path=model_dir,\n",
    "            device_map='auto',\n",
    "            trust_remote_code=True,\n",
    "            # torch_dtype=torch.bfloat16,\n",
    "            # temperature=0.1\n",
    "            )\n",
    "        self.model = self.model.eval()\n",
    "        self.model.generation_config = GenerationConfig.from_pretrained(\n",
    "            model_dir,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        # 可指定不同的生成长度、top_p等相关超参\n",
    "        self.processor = AutoProcessor.from_pretrained(model_dir)\n",
    "        print('模型加载完成！')\n",
    "\n",
    "    def _call(self, messages,\n",
    "              stop: Optional[List[str]] = None,\n",
    "              run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "              **kwargs: Any):\n",
    "        # response, history = self.model.chat(self.tokenizer, prompt, history=[])\n",
    "        if type(messages) == str:\n",
    "            system_info =  messages.split('System:')[-1].split('Human:')[0].strip()\n",
    "            human_info =  messages.split('Human:')[-1].strip()\n",
    "            re_prompt = [\n",
    "                    {'role':'system', 'content': system_info},\n",
    "                    {'role':'user', 'content': human_info}\n",
    "                    ]\n",
    "            messages = re_prompt\n",
    "        # print('打印messages内容：', messages)\n",
    "        # print('打印messages类型：', type(messages))\n",
    "        text = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        model_inputs = self.tokenizer([text], return_tensors='pt').to('cpu')\n",
    "        generated_ids = self.model.generate(\n",
    "            **model_inputs,\n",
    "            max_new_tokens=512\n",
    "        )\n",
    "        generated_ids = [\n",
    "            output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "        response = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "        # print('response, messages', response, '\\n', messages)\n",
    "        return response\n",
    "        \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"QwenLM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f6fd3130-b11d-492c-98c2-9a046aff37fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 构建改写问题prompt\n",
    "\n",
    "# 请根据聊天历史和最后用户的问题，改写用户最终剔除的问题。\n",
    "# 你只需要改写用户最终的问题，请不要回答问题。\n",
    "# 没有聊天历史则将用户问题直接返回，有聊天历史则改写。\n",
    "    \n",
    "def contextualize_question_prompt():\n",
    "    system_prompt = \"\"\"\\\n",
    "    根据聊天历史和输入文本，改写输入文本。请根据工作流程和工作要求改写文本。\n",
    "\n",
    "    工作流程：\n",
    "    1-判断聊天历史是否为空。内容为空直接返回输入文本，否则继续以下工作流程。\n",
    "    2-聊天历史存在内容，请根据聊天历史和输入文本内容相关程度，改写输入文本。如果相关程度低则直接输出文本。\n",
    "    3-请思考改写后的输入文本内容，是否突出文本意图，否则重新根据输入文本改写。请注意如果是打招呼等日常用语也直接返回输入文本。\n",
    "    4-请不要回答输入的文本，输出改写的文本内容。\n",
    "\n",
    "    工作要求：\n",
    "    1-你只需要改写文本。\n",
    "    2-聊天历史为空则将输入文本直接返回，有聊天历史则改写。    \n",
    "    \"\"\"\n",
    "    contextualize_question_prompt = ChatPromptTemplate(\n",
    "        [\n",
    "        ('system', system_prompt),\n",
    "        MessagesPlaceholder('chat_history'),\n",
    "        ('human', '{input}')\n",
    "        ]\n",
    "    )\n",
    "    return contextualize_question_prompt\n",
    "\n",
    "def answer_prompt():\n",
    "    # 构建正常问答prompt\n",
    "    system_prompt = \"\"\"\\\n",
    "    使用上下文来回答最后的问题。如果你不知道答案，就说你不知道，不要试图编造答案。\n",
    "    {context}\n",
    "    \"\"\"\n",
    "    # 问题: {question}\n",
    "    # 有用的回答:\n",
    "    \n",
    "    qa_prompt = ChatPromptTemplate(\n",
    "    [\n",
    "    ('system', system_prompt),\n",
    "        MessagesPlaceholder('chat_history'),\n",
    "        ('human', '{input}')\n",
    "    ]\n",
    "    )\n",
    "    return qa_prompt\n",
    "\n",
    "store = {}\n",
    "# 构造存储函数\n",
    "def get_session_history(session_id: str):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0d379e00-210e-4884-b465-f1a772301df5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_model():\n",
    "    # 初始化模型\n",
    "    # model_cache_path = r'autodl-tmp/Qwen/Qwen-7B-Chat'\n",
    "    model_cache_path = r'autodl-tmp/LLM-Research/gemma-2-9b-it'\n",
    "    llm = QianWenLLM(model_dir=model_cache_path)\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "68462e5d-7a04-4994-b38d-661abc216427",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_db():\n",
    "    persist_directory = r'vectordb/chroma'\n",
    "    embeddings_model_cache_path = r'autodl-tmp/embedding_model/Ceceliachenen/paraphrase-multilingual-MiniLM-L12-v2'\n",
    "    # 加载词向量模型\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=embeddings_model_cache_path)\n",
    "    # 加载缓存知识库\n",
    "    vectordb = Chroma(\n",
    "        persist_directory=persist_directory,\n",
    "        embedding_function=embeddings,\n",
    "    )\n",
    "    return vectordb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d10f82cf-4182-4d77-8a06-296662d2caa3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def intention_recognition(question: str, llm):\n",
    "    template = f\"\"\"\n",
    "        你是一名实体提取和意图识别分类领域专家，请严格遵循以下任务和工作流程的指示输出结果。\n",
    "        \n",
    "        任务：\n",
    "        1-判断用户问题是否存在实体。\n",
    "        2-抽取用户问题所有实体。\n",
    "        3-根据实体与给出的意图标签进行判定该用户问题的意图。\n",
    "        \n",
    "        意图标签：政策知识，日常知识，招中标知识\n",
    "        \n",
    "        工作流程：\n",
    "        1.-先判断是否存在实体，不存在实体则直接根据不存在实体输出格式输出，存在实体则继续以下工作流程，并通过存在实体输出格式输出。\n",
    "        2-实体提取：请从用户问题中提取出所有实体。\n",
    "        3-意图分类：请根据第2点提取的实体以及意图标签，进行意图识别并分类用户问题。\n",
    "        \n",
    "        不存在实体输出格式：\n",
    "            实体提取:[]\n",
    "            意图分类:日常知识\n",
    "            \n",
    "        存在实体输出格式：\n",
    "            实体提取：[实体1, 实体2, ...]\n",
    "            意图分类：意图标签\n",
    "            \n",
    "        有用的回答：\n",
    "       \"\"\"\n",
    "          # 4. 意图分类：根据第3点的意图识别结果分类，意图标签：[政策知识，日常知识，招中标知识]。       2.意图识别：[意图1, 意图2, ...]。\n",
    "    # ，例如日常、政策、设备、建筑、维修等\n",
    "    messages = [\n",
    "            {'role':'system', 'content': template},\n",
    "            {'role':'user', 'content':question}]\n",
    "\n",
    "    response = llm._call(messages)\n",
    "    # print('实体+意图模型结果：', response)\n",
    "    target = response.split('意图分类')[-1]\n",
    "    # print('模型实体+意图结果：', response)\n",
    "    print('意图分类:', target)\n",
    "    return target\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2b9f8397-d128-4dbc-a209-746e34a9828b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def qa_chain(rag_chain, get_session_history):\n",
    "    \"\"\"\n",
    "    构建问答链\n",
    "    :param persist_directory: 知识库本地保存路径，这里初始化政策信息知识库\n",
    "    :return: 返回调用LLM回答\n",
    "    \"\"\"\n",
    "    # 包装\n",
    "    conversational_rag_chain = RunnableWithMessageHistory(\n",
    "        runnable=rag_chain,\n",
    "        get_session_history=get_session_history,\n",
    "        input_messages_key='input',\n",
    "        history_messages_key='chat_history',\n",
    "        output_messages_key='answer',\n",
    "    )\n",
    "\n",
    "    return conversational_rag_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3ce3be43-5e17-4dcc-8baa-c07ce17c75b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def contextualize_question_chain(llm):\n",
    "    # 根据历史检索器原理，我们这里只要llm和改写prompt和历史记录就行\n",
    "    question_prompt = contextualize_question_prompt()\n",
    "    contextualize_question_chain = RunnableWithMessageHistory(\n",
    "        question_prompt | llm,\n",
    "        get_session_history,\n",
    "        input_messages_key=\"input\",\n",
    "        history_messages_key=\"chat_history\"\n",
    "    )\n",
    "    return contextualize_question_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "333bcc19-cc3e-42c7-8655-8fa8630b372c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 链接前面的函数\n",
    "class Model_center():\n",
    "    \"\"\"\n",
    "      存储问答 Chain 的对象\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ):\n",
    "        print('初始化模型+知识库。。。。')\n",
    "        self.model = init_model()\n",
    "        self.db = init_db()\n",
    "         # 因为历史记录问题进行问题改写再传到模型\n",
    "        self.question_prompt = contextualize_question_prompt()\n",
    "        # 历史对话巡回器\n",
    "        self.history_aware_retriever = create_history_aware_retriever(self.model, \n",
    "                                                                self.db.as_retriever(), \n",
    "                                                                 self.question_prompt)\n",
    "        self.qa_prompt = answer_prompt()\n",
    "        self.qa_chain = create_stuff_documents_chain(self.model, self.qa_prompt)\n",
    "        self.rag_chain = create_retrieval_chain(self.history_aware_retriever, self.qa_chain)\n",
    "        \n",
    "\n",
    "    def qa_chain_self_answer(self, question: str, chat_history: list = []):\n",
    "        # print('调用问答链')\n",
    "        # print('打印用户问题', question)\n",
    "        if question == None or len(question) < 1:\n",
    "            print('问答为空。。。。')\n",
    "            return '', chat_history\n",
    "        print('实体提取+意图识别。。。。')\n",
    "\n",
    "        # for question in sentence:\n",
    "        # try\n",
    "        # print('用户输入问题：', question)\n",
    "        target = intention_recognition(question, self.model)\n",
    "        # 只提取中文\n",
    "        def isChinese(target):\n",
    "            for ch in target: \n",
    "                if not '\\u4e00' <= ch <= '\\u9fff':\n",
    "                    return False\n",
    "            return True \n",
    "        if not isChinese(target):\n",
    "            target=re.sub('[^\\u4e00-\\u9fa5]+','',target)\n",
    "            print('意图识别存在非中文字符，调整后：', target)\n",
    "        print('意图识别成功。。。。')\n",
    "        # except:\n",
    "            # print('实体提取+意图识别步骤失败。。。。')\n",
    "\n",
    "\n",
    "        if target == '招中标知识' or target == '政策知识':\n",
    "            print('调用检索问答链。。。。')\n",
    "            # print('知识库:', target)\n",
    "            # 带langchain实力对话共能\n",
    "            response = qa_chain(\n",
    "                self.rag_chain,\n",
    "                get_session_history).invoke(\n",
    "                    {'input': question},\n",
    "                    config={'configurable':{'session_id':'test123'}}\n",
    "                    )\n",
    "            call = response['context']\n",
    "            answer = response['answer']\n",
    "            new_question = response['input']\n",
    "            print('改写前的问题：', question, '改写后的问题：', new_question)\n",
    "            print(f'意图分类：{target}，测试问题：{question}，返回结果：{answer}')\n",
    "            # print('调用检索问答链输出：', response)\n",
    "            # call = messages[0]['content'].split('不要试图编造答案。').strip()\n",
    "            # print('检索链call：', call)\n",
    "\n",
    "        else:\n",
    "            \n",
    "            print('调用简单模型。。。。')\n",
    "            \n",
    "            # 改写问题\n",
    "            \n",
    "            # 将改写的新问题输入进行回答，改写的记录也会加入聊天历史中\n",
    "            new_question = contextualize_question_chain(self.model).invoke( \n",
    "                {'input': question},\n",
    "                config={'configurable':{'session_id':'test123'}}\n",
    "                    )\n",
    "            # print('用户问题：', question)\n",
    "            # print('根据历史聊天记录改写用户问题：', new_question)\n",
    "\n",
    "            template = \"\"\"\\\n",
    "                        请回答用户的问题，如果你不知道答案，就说你不知道，不要试图编造答案。\n",
    "                    \"\"\"\n",
    "            messages = [\n",
    "                {'role':'system', 'content': template},\n",
    "                {'role':'user', 'content':new_question}\n",
    "                ]\n",
    "            response = self.model._call(messages)\n",
    "            store['test123'].add_user_message(question)\n",
    "            store['test123'].add_ai_message(response)\n",
    "            call = ''\n",
    "            print(f'意图分类：{target}，测试问题：{question}，返回结果：{response}')\n",
    "            print('改写前的问题：', question, '改写后的问题：', new_question)\n",
    "        return target, new_question, response, call\n",
    "        \n",
    "        # 检索问答链+历史聊天组件\n",
    "        # response = self.qa_chain.invoke({'query': question, 'chat_history': chat_history})['answer']\n",
    "\n",
    "        # chat_history.append([question, response])\n",
    "        # print(chat_history)\n",
    "        # return '', chat_history\n",
    "                \n",
    "        # except Exception as e:\n",
    "        #     print('问答链报错', e)\n",
    "        #     return e, chat_history\n",
    "\n",
    "    def clear_history(self):\n",
    "        self.qa_chain.clear_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abcb68b-dbaa-4139-941b-37014e7d7c2a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化模型+知识库。。。。\n",
      "正从本地加载模型。。。。。\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef0810399f464aab8f4e8aafaeab6e82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "model_center = Model_center()\n",
    "# 加载数据文本，提取问题和真实数据源\n",
    "test_res = []\n",
    "qa_data = pd.read_excel('问答对.xlsx').iloc[:, 2:]\n",
    "length = len(qa_data.iloc[0, :])\n",
    "history = []\n",
    "# target_list = []\n",
    "# new_question_list = []\n",
    "# response_list = []\n",
    "# call = []\n",
    "file = 'qwen2.5模型测试结果.csv'\n",
    "data_res = pd.DataFrame([], columns=['Question','Answer', 'reference', 'Metadata', 'Text', '意图分类','问题改写', '回答', 'RAG召回'])\n",
    "if os.path.exists(file):\n",
    "    data_tested = pd.read_csv(file, encoding='utf-8')['Question'].tolist()\n",
    "else:\n",
    "    data_tested = []\n",
    "for q in range(len(qa_data)):\n",
    "    question = qa_data.iloc[q, 0]\n",
    "    \n",
    "    if question not in data_tested:\n",
    "        print('打印问题:', question)\n",
    "        target, new_question, response, call = model_center.qa_chain_self_answer(question, history)\n",
    "        data_ = qa_data.iloc[q].values.tolist() + [target] + [new_question] + [response] + [call]\n",
    "        with open(file, 'a', newline='', encoding='utf-8') as f:\n",
    "            fw = csv.writer(f)\n",
    "            if not os.path.getsize(file):\n",
    "                header = ['Question', 'Answer', 'reference', 'Metadata', 'Text', '意图分类', '问题改写', '回答', 'RAG召回']\n",
    "                fw.writerow(header)\n",
    "            fw.writerow(data_)\n",
    "    #         # 写入标题行\n",
    "    # writer.writerow(headers)\n",
    "    # data_.to_excel('qwen2.5模型测试结果.xlsx', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa35931c-ac49-44fd-aa81-a3a782af46a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentence = [\n",
    "            '你好',\n",
    "            '你是谁？',\n",
    "            '上海联合产权交易所有限公司物权交易操作指引(沪联产交〔2020〕26号)',\n",
    "            '周杰伦是谁？',\n",
    "            '产权交易争议调解',\n",
    "            '2023年7月，中国航天科技集团公司五院研制的长征十二号运载火箭在酒泉卫星发射中心成功发射，将一颗北斗导航卫星送入预定轨道。',\n",
    "            '周杰伦的歌曲《稻香》是谁写的？',\n",
    "            '项目名称为ABC，中标金额为500万元，供应商为XYZ公司。']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3966af8-c640-48ba-a724-96b0e6e0d008",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = f\"\"\"\n",
    "        你是一名实体提取和意图识别分类领域专家，请严格遵循以下任务和工作流程的指示输出结果。\n",
    "        \n",
    "        任务：\n",
    "        1-判断输入文本是否存在实体。\n",
    "        2-抽取输入文本所有实体。\n",
    "        3-根据实体与给出的意图标签进行判定该输入文本的意图。\n",
    "        \n",
    "        意图标签：政策知识，日常知识，招中标知识\n",
    "        \n",
    "        工作流程：\n",
    "        1.-先判断是否存在实体，不存在实体则直接根据不存在实体输出格式输出，存在实体则继续以下工作流程，并通过存在实体输出格式输出。\n",
    "        2-实体提取：请从输入的文本中提取出所有实体。\n",
    "        3-意图分类：请根据第2点提取的实体以及意图标签，进行意图识别并分类输入文本。\n",
    "        \n",
    "        输入文本：{question}\n",
    "        不存在实体输出格式：\n",
    "            实体提取:[]\n",
    "            意图分类:日常知识\n",
    "        存在实体输出格式：\n",
    "            实体提取：[实体1, 实体2, ...]\n",
    "            意图分类：意图标签\n",
    "\n",
    "        有用的回答：\n",
    "       \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ace048e1-5191-4166-be97-729782208350",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chat_history_' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mchat_history_\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'chat_history_' is not defined"
     ]
    }
   ],
   "source": [
    "question_prompt = f\"\"\"\\\n",
    "    根据聊天历史和用户问题，改写用户问题。请根据工作流程和工作要求改写问题。\n",
    "\n",
    "    工作流程：\n",
    "    1-判断聊天历史是否为空。内容为空直接返回用户的问题，否则继续以下工作流程。\n",
    "    2-聊天历史存在内容，请根据聊天历史和用户的问题，改写用户提出的问题。\n",
    "    3-请思考改写后的问题内容，是否突出用户问题意图，否则重新根据用户问题改写。请注意如果是打招呼等日常用语也直接返回用户问题。\n",
    "    4-请不要回答用户的问题，输出改写的问题。\n",
    "\n",
    "    工作要求：\n",
    "    1-你只需要改写用户的问题。\n",
    "    2-聊天历史为空则将用户问题直接返回，有聊天历史则改写。                         \n",
    "\n",
    "    聊天历史：{chat_history_}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d39910ea-f7ef-4bb1-98ea-6bfbeb445b9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def history():\n",
    "    MessagesPlaceholder('chat_history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d4078d7-c2ff-4691-a187-051ec0961e8f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['如果投标文件解密失败，招标人是否有义务提供补救方案？',\n",
       " '如果投标文件解密失败，招标人是否有义务提供补救方案？',\n",
       " '如果投标文件解密失败，招标人是否有义务提供补救方案？']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = 'qwen2.5模型测试结果.csv'\n",
    "data_tested = pd.read_csv(file, encoding='utf-8')['Question'].tolist()\n",
    "data_tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eeb27e3f-475b-490a-88e0-8b687be6c6c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "意图识别存在非中文字符，调整后： 招中标知识公示和公布要求\n"
     ]
    }
   ],
   "source": [
    "# 只提取中文\n",
    "target = \"'_'.join(['招中标知识', '公示和公布要求'])\"\n",
    "def isChinese(target):\n",
    "    for ch in target: \n",
    "        if not '\\u4e00' <= ch <= '\\u9fff':\n",
    "            return False\n",
    "    return True \n",
    "if not isChinese(target):\n",
    "    target=re.sub('[^\\u4e00-\\u9fa5]+','',target)\n",
    "    print('意图识别存在非中文字符，调整后：', target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea7c832-1464-40af-99ab-bb869b113758",
   "metadata": {},
   "outputs": [],
   "source": [
    "['Question']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
