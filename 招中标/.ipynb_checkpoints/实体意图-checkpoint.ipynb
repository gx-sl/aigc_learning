{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4ad5bf67-3e97-4ad6-801a-ef63e5cd6770",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from typing import Optional, List, Any\n",
    "import torch\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig, TextIteratorStreamer, AutoProcessor\n",
    "# from modelscope import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.outputs.generation import GenerationChunk\n",
    "from torch import device\n",
    "import time\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "\n",
    "\n",
    "from threading import Thread\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6cb65caa-1919-4599-bc5d-9023aec1081c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 构建模型\n",
    "class QianWenLLM(LLM):\n",
    "    # 基于本地的QianWen7B-Chat模型自定义LLM类\n",
    "    tokenizer: AutoTokenizer = None\n",
    "    model: AutoModelForCausalLM = None\n",
    "    processor: AutoProcessor = None\n",
    "    \n",
    "    def __init__(self, model_dir: str):\n",
    "        # 从本地加载模型\n",
    "        super().__init__()\n",
    "        print('正从本地加载模型。。。。。')\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            pretrained_model_name_or_path=model_dir,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            pretrained_model_name_or_path=model_dir,\n",
    "            device_map='auto',\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            # temperature=0.1\n",
    "            )\n",
    "        self.model = self.model.eval()\n",
    "        self.model.generation_config = GenerationConfig.from_pretrained(\n",
    "            model_dir,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        # 可指定不同的生成长度、top_p等相关超参\n",
    "        self.processor = AutoProcessor.from_pretrained(model_dir)\n",
    "        print('模型加载完成！')\n",
    "\n",
    "    def _call(self, prompt: str,\n",
    "              stop: Optional[List[str]] = None,\n",
    "              run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "              **kwargs: Any):\n",
    "        # print('_call函数内查看prompt', prompt)\n",
    "        response, history = self.model.chat(self.tokenizer, prompt, history=[])\n",
    "        # response, history = self.model.generate(self.tokenizer, prompt, history=[])\n",
    "        return response\n",
    "        \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"QwenLM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0d379e00-210e-4884-b465-f1a772301df5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_model():\n",
    "    # 初始化模型\n",
    "    model_cache_path = r'autodl-tmp/Qwen/Qwen-7B-Chat'\n",
    "    # model_cache_path = r'autodl-tmp/Qwen/Qwen2.5-7B-Instruct'\n",
    "    llm = QianWenLLM(model_dir=model_cache_path)\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "68462e5d-7a04-4994-b38d-661abc216427",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_db():\n",
    "    persist_directory = r'vectordb/chroma'\n",
    "    embeddings_model_cache_path = r'autodl-tmp/embedding_model/Ceceliachenen/paraphrase-multilingual-MiniLM-L12-v2'\n",
    "    # 加载词向量模型\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=embeddings_model_cache_path)\n",
    "    # 加载缓存知识库\n",
    "    vectordb = Chroma(\n",
    "        persist_directory=persist_directory,\n",
    "        embedding_function=embeddings,\n",
    "    )\n",
    "    return vectordb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d10f82cf-4182-4d77-8a06-296662d2caa3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def intention_recognition(question: str, llm):\n",
    "    template = f\"\"\"\n",
    "        你是一名实体提取和意图识别分类领域专家，请严格遵循以下任务和工作流程的指示输出结果。\n",
    "        \n",
    "        任务：\n",
    "        1-判断输入文本是否存在实体。\n",
    "        2-抽取输入文本所有实体。\n",
    "        3-根据实体与给出的意图标签进行判定该输入文本的意图。\n",
    "        \n",
    "        意图标签：政策知识，日常知识，招中标知识\n",
    "        \n",
    "        工作流程：\n",
    "        1-先判断是否存在实体，不存在实体则直接根据不存在实体输出格式输出，存在实体则继续以下工作流程，并通过存在实体输出格式输出。\n",
    "        2-实体提取：请从输入的文本中提取出所有实体。\n",
    "        3-意图分类：请根据第2点提取的实体以及意图标签，进行意图识别并分类输入文本。\n",
    "        \n",
    "        输入文本：{question}\n",
    "        \n",
    "        不存在实体输出格式：\n",
    "            实体提取:[]\n",
    "            意图分类:日常知识\n",
    "            \n",
    "        存在实体输出格式：\n",
    "            实体提取：[实体1, 实体2, ...]\n",
    "            意图分类：意图标签\n",
    "\n",
    "        有用的回答：\n",
    "       \"\"\"\n",
    "          # 4. 意图分类：根据第3点的意图识别结果分类，意图标签：[政策知识，日常知识，招中标知识]。       2.意图识别：[意图1, 意图2, ...]。\n",
    "    # ，例如日常、政策、设备、建筑、维修等\n",
    "    response = llm.invoke(input=template.format(question))\n",
    "    target = response.split('意图分类：')[-1]\n",
    "    # print('模型实体+意图结果：', response)\n",
    "    # print('意图分类:', target)\n",
    "    return target\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2b9f8397-d128-4dbc-a209-746e34a9828b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def qa_chain(vectordb, llm):\n",
    "    \"\"\"\n",
    "    构建问答链\n",
    "    :param persist_directory: 知识库本地保存路径，这里初始化政策信息知识库\n",
    "    :return: 返回调用LLM回答\n",
    "    \"\"\"\n",
    "    template = \"\"\"\n",
    "                使用上下文来回答最后的问题。如果你不知道答案，就说你不知道，不要试图编造答案。总是在回答的最后说”谢谢你的提问！”。\n",
    "                {context}\n",
    "                问题：{question}\n",
    "                有用的回答：\n",
    "                \"\"\"\n",
    "    # 尽量使答案简明扼要。\n",
    "    # 调用 LangChain 的方法来实例化一个 Template 对象，该对象包含了 context 和 question 两个变量，\n",
    "    # 在实际调用时，这两个变量会被检索到的文档片段和用户提问填充\n",
    "    QA_CHAIN_PROMPT = PromptTemplate(input_variables=['context', 'question'], template=template)\n",
    "\n",
    "    # 构造检索问答链\n",
    "    qa_chain_ = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        retriever=vectordb.as_retriever(),\n",
    "        return_source_documents=True,\n",
    "        chain_type_kwargs={'prompt': QA_CHAIN_PROMPT},\n",
    "        # chain_type='stuff'\n",
    "    )\n",
    "\n",
    "    return qa_chain_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "333bcc19-cc3e-42c7-8655-8fa8630b372c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 链接前面的函数\n",
    "class Model_center():\n",
    "    \"\"\"\n",
    "      存储问答 Chain 的对象\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ):\n",
    "        print('初始化模型+知识库。。。。')\n",
    "        self.model = init_model()\n",
    "        self.db = init_db()\n",
    "        \n",
    "\n",
    "    def qa_chain_self_answer(self, question: str, chat_history: list = []):\n",
    "        # print('调用问答链')\n",
    "        # print('打印用户问题', question)\n",
    "        if question == None or len(question) < 1:\n",
    "            print('问答为空。。。。')\n",
    "            return '', chat_history\n",
    "        print('实体提取+意图识别。。。。')\n",
    "\n",
    "        # for question in sentence:\n",
    "        try:\n",
    "            target = intention_recognition(question, self.model)\n",
    "            print('意图识别成功。。。。')\n",
    "        except:\n",
    "            print('实体提取+意图识别步骤失败。。。。')\n",
    "            \n",
    "        if target == '招中标知识' or target == '政策知识':\n",
    "            print('调用检索问答链。。。。')\n",
    "            # print('知识库:', target)\n",
    "            response = qa_chain(self.db, self.model).invoke({'query': question})['result']\n",
    "        else:\n",
    "            print('调用简单模型。。。。')\n",
    "            template = f\"\"\"\n",
    "                        回答输入的问题，如果你不知道答案，就说你不知道，不要试图编造答案。\n",
    "                        输入的问题：{question}\n",
    "                        有用的回答：\n",
    "                        \"\"\"\n",
    "            response = self.model.invoke(input=template.format(question))\n",
    "        print(f'意图分类：{target}，测试问题：{question}，返回结果：{response}')\n",
    "        # try:\n",
    "        \n",
    "        # 结果调用下流式输出    target == '日常知识':\n",
    "        \n",
    "        \n",
    "        # 检索问答链+历史聊天组件\n",
    "        # response = self.qa_chain.invoke({'query': question, 'chat_history': chat_history})['answer']\n",
    "\n",
    "        chat_history.append([question, response])\n",
    "        # print(chat_history)\n",
    "        return '', chat_history\n",
    "                \n",
    "        # except Exception as e:\n",
    "        #     print('问答链报错', e)\n",
    "        #     return e, chat_history\n",
    "\n",
    "    def clear_history(self):\n",
    "        self.qa_chain.clear_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7abcb68b-dbaa-4139-941b-37014e7d7c2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化模型+知识库。。。。\n",
      "正从本地加载模型。。。。。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to \"AutoModelForCausalLM.from_pretrained\".\n",
      "Try importing flash-attention for faster inference...\n",
      "Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\n",
      "Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n",
      "Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b9c9836670f4619b443ec860c18bb1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "The repository for autodl-tmp/Qwen/Qwen-7B-Chat contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/autodl-tmp/Qwen/Qwen-7B-Chat.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n",
      "The repository for autodl-tmp/Qwen/Qwen-7B-Chat contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/autodl-tmp/Qwen/Qwen-7B-Chat.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型加载完成！\n",
      "进度1\n",
      "进度2\n",
      "Running on local URL:  http://127.0.0.1:6018\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:6018/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPORTANT: You are using gradio version 3.50.2, however version 4.44.1 is available, please upgrade.\n",
      "--------\n",
      "实体提取+意图识别。。。。\n",
      "意图识别成功。。。。\n",
      "调用简单模型。。。。\n",
      "意图分类：日常知识，测试问题：你好，返回结果：你好！有什么我能帮你的吗？\n",
      "实体提取+意图识别。。。。\n",
      "意图识别成功。。。。\n",
      "调用简单模型。。。。\n",
      "意图分类：日常知识，测试问题：你是谁？，返回结果：我是通义千问，由阿里云开发的预训练语言(si)模型。我被设计用来回答各种问题、提供信息和与用户进行对话。如果您有任何问题，请随时问我！\n",
      "实体提取+意图识别。。。。\n",
      "意图识别成功。。。。\n",
      "调用简单模型。。。。\n",
      "意图分类：日常知识，测试问题：周杰伦的父亲是谁？，返回结果：周杰伦的父亲是周耀中。\n",
      "实体提取+意图识别。。。。\n",
      "意图识别成功。。。。\n",
      "调用检索问答链。。。。\n",
      "意图分类：政策知识，测试问题：我想了解上海市奉贤区农村集体产权交易规则的具体内容，你知道吗？，返回结果：很抱歉，我没有找到关于上海市奉贤区农村集体产权交易规则的具体内容的信息。您可以在相关的官方网站或文件中查找相关信息，或者咨询当地的政府部门。\n",
      "实体提取+意图识别。。。。\n",
      "意图识别成功。。。。\n",
      "调用检索问答链。。。。\n",
      "意图分类：政策知识，测试问题：上海联合产权交易所有限公司物权交易操作指引的具体要求你知道吗，返回结果：是的，我知道上海联合产权交易所有限公司物促成权交易操作指引的具体要求。该指引是为了规范在联交所开展的物权交易活动，进一步明确针对物权交易特性的工作流程和管理操作标准，更好地开展物权交易市场化运作，发现资产价格，根据相关规定制定的。\n",
      "实体提取+意图识别。。。。\n",
      "意图识别成功。。。。\n",
      "调用检索问答链。。。。\n",
      "意图分类：政策知识，测试问题：请展示《上海联合产权交易所有限公司物权交易操作指引》的具体内容，返回结果：第一条为规范在上海联合产权交易所有限公司（以下简称“联交所”）开展的物权交易活动，进一步明确针对物权交易特性的工作流程和管理操作标准，更好地开展物权交易市场化运作，发现资产价格，根据相关规定制定本指引。\n",
      "\n",
      "第二条在联交所内开展的物权交易是指《中华人民共和国物权法》规定的物的所有权以及用益物权的转让。法律、法规、规章、规范性文件或联交所交易规则对某类物的物权转让有特别规定，从其规定。\n",
      "\n",
      "第三条转让方应为物权权利持有人、依据法律法规或合同约定拥有实际处分权利的主体。\n",
      "\n",
      "第四条意向受让方应为有受让意向且符合受让资格条件的主体。\n",
      "\n",
      "第五条转让方向联交所提出交易申请的，应与联交所签订交易服务合同。转让方委托联交所认可的受托机构办理物权交易手续的，应与受托机构签订交易委托服务合同。\n",
      "\n",
      "第六条转让申请内容包括物权标的基本情况、标的评估或估值情况、与转让相关的条件、交易方式等。转让方应根据联交所的要求提供以下转让申请附件材料：1．产权证；2．营业执照；3．税务登记证；4．土地使用权证；5．公司章程等。\n",
      "\n",
      "第七条转让方应按照联交所的规定，对拟转让的物权进行全面清查，并确保提供的资料真实有效。转让方应对拟转让的物权进行合理估价，并出具相应的评估报告。\n",
      "\n",
      "第八条意向受让方应按照联交所的规定，向转让方支付一定的意向金，并提供身份证明和资信证明。意向受让方有权要求转让方对其拟受让的物权进行现场查看，并在约定的时间内对拟受让的物权进行详细考察。\n",
      "\n",
      "第九条意向受让方应在指定的时间内向联交所提交受让申请书，并附上意向金付款凭证和身份证明和资信证明。\n",
      "\n",
      "第十条意向受让方应按照联交所的规定，对拟受让的物权进行详细考察，并在约定的时间内完成受让。\n",
      "\n",
      "第十一条转让方应按照联交所的规定，对意向受让方提交的受让申请进行审查，并在收到受让申请后的五个工作日内作出书面答复。\n",
      "\n",
      "第十二条转让方和受让方应按照联交所的规定，签订《\n"
     ]
    }
   ],
   "source": [
    "model_center = Model_center()\n",
    "def update_chatbot(question, chat_history):\n",
    "    for char in model_center.qa_chain_self_answer(question, chat_history):\n",
    "        gr.update(value=chat_history)\n",
    "        chat_history.append((question, char))\n",
    "    return chat_history\n",
    "# def demo():\n",
    "block = gr.Blocks()\n",
    "with block as demo:\n",
    "    with gr.Row(equal_height=True):  # 水平排列子组件\n",
    "        with gr.Column(scale=15):  # 垂直排列子组件\n",
    "            gr.Markdown(\"\"\"<h1><center>QwenLM7B-Chat</center></h1><center>科大讯飞实践-招中标政策智能问答助手</center>\"\"\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=4):\n",
    "            # 创建聊天界面的组件。height=450 参数设置了聊天界面的高度为 450 像素。\n",
    "            # show_copy_button=True参数表示在聊天界面中显示一个复制按钮，允许用户复制聊天内容\n",
    "            chatbot = gr.Chatbot(height=450, show_copy_button=True)\n",
    "            # 创建一个文本框组件，用于输入 prompts。\n",
    "            msg = gr.Textbox(label='Prompt/问题')\n",
    "\n",
    "            with gr.Row():\n",
    "                # 创建提交按钮\n",
    "                db_wo_his_btn = gr.Button('Chat')\n",
    "            with gr.Row():\n",
    "                # 创建一个清除按钮，用于清除聊天机器人组件的内容。\n",
    "                clear_btn = gr.ClearButton(components=[chatbot], value='Clear console')\n",
    "\n",
    "        # 设置按钮的点击事件。当点击时，调用上面定义的 qa_chain_self_answer 函数，并传入用户的消息和聊天历史记录，然后更新文本框和聊天机器人组件。\n",
    "        print('进度1')\n",
    "        # 设置流式输出\n",
    "        def bot(question, history):\n",
    "            # print('bot_question',question)\n",
    "            # print('bot_history',history)\n",
    "            curr, response = model_center.qa_chain_self_answer(question, history)\n",
    "            # print('response', response)\n",
    "            # print('curr', curr)\n",
    "            history = response\n",
    "            bot_message = history[-1][1]\n",
    "            # print('bot_message', bot_message)\n",
    "            history[-1][1] = ''\n",
    "            for character in bot_message:\n",
    "                history[-1][1] += character\n",
    "                # print(f'累计中：{history}')\n",
    "                time.sleep(0.1)\n",
    "                yield '', history\n",
    "        \n",
    "        db_wo_his_btn.click(bot, inputs=[msg, chatbot], outputs=[msg, chatbot])\n",
    "\n",
    "        print('进度2')\n",
    "        # 点击后清空后端存储的聊天记录\n",
    "        clear_btn.click(model_center.clear_history)\n",
    "\n",
    "    # 填写注意事项\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        提醒：<br>\n",
    "        1. 初始化数据库实践可能较长，请耐心等待。\n",
    "        2. 使用中如果出现异常，将会在文本输入框进行展示，请不要惊慌。 <br>\n",
    "        \"\"\"\n",
    "    )\n",
    "# gr.close_all()\n",
    "# 直接启动\n",
    "demo.queue()\n",
    "demo.launch(server_name='127.0.0.1', server_port=6018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa35931c-ac49-44fd-aa81-a3a782af46a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentence = [\n",
    "            '你好',\n",
    "            '你是谁？',\n",
    "            '上海联合产权交易所有限公司物权交易操作指引(沪联产交〔2020〕26号)',\n",
    "            '周杰伦是谁？',\n",
    "            '产权交易争议调解',\n",
    "            '2023年7月，中国航天科技集团公司五院研制的长征十二号运载火箭在酒泉卫星发射中心成功发射，将一颗北斗导航卫星送入预定轨道。',\n",
    "            '周杰伦的歌曲《稻香》是谁写的？',\n",
    "            '项目名称为ABC，中标金额为500万元，供应商为XYZ公司。']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3966af8-c640-48ba-a724-96b0e6e0d008",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = f\"\"\"\n",
    "        你是一名实体提取和意图识别分类领域专家，请严格遵循以下任务和工作流程的指示输出结果。\n",
    "        \n",
    "        任务：\n",
    "        1-判断输入文本是否存在实体。\n",
    "        2-抽取输入文本所有实体。\n",
    "        3-根据实体与给出的意图标签进行判定该输入文本的意图。\n",
    "        \n",
    "        意图标签：政策知识，日常知识，招中标知识\n",
    "        \n",
    "        工作流程：\n",
    "        1.-先判断是否存在实体，不存在实体则直接根据不存在实体输出格式输出，存在实体则继续以下工作流程，并通过存在实体输出格式输出。\n",
    "        2-实体提取：请从输入的文本中提取出所有实体。\n",
    "        3-意图分类：请根据第2点提取的实体以及意图标签，进行意图识别并分类输入文本。\n",
    "        \n",
    "        输入文本：{question}\n",
    "        不存在实体输出格式：\n",
    "            实体提取:[]\n",
    "            意图分类:日常知识\n",
    "        存在实体输出格式：\n",
    "            实体提取：[实体1, 实体2, ...]\n",
    "            意图分类：意图标签\n",
    "\n",
    "        有用的回答：\n",
    "       \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
