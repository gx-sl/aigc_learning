{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0e7d96c-45b4-463e-be50-3f7aecbd637e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping directory: data/policy/.ipynb_checkpoints\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/root/miniconda3/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from typing import List, Tuple\n",
    "\n",
    "# from langchain_community.vectorstores import Milvus\n",
    "# from langchain_milvus import Milvus\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "# from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_milvus.utils.sparse import BM25SparseEmbedding\n",
    "from langchain_milvus.vectorstores import Milvus\n",
    "\n",
    "json_folder_path = 'data/policy'\n",
    "embedding_model_path = r\"autodl-tmp/embedding_model/BAAI/bge-large-zh-v1___5\"\n",
    "persist_directory = 'vectordb/milvus_mix'\n",
    "\n",
    "# 从文件夹读取文件名称列表，获取所有的文件完整路径\n",
    "def get_file_name_form_folder(json_folder_path: str) -> List[str]:\n",
    "    file_path_list_all = []\n",
    "    for file_name in os.listdir(json_folder_path):\n",
    "        file_path = os.path.join(json_folder_path, file_name)\n",
    "        file_path_list_all.append(file_path)\n",
    "    return file_path_list_all\n",
    "\n",
    "\n",
    "# 读取和解析 JSON 文件\n",
    "def parse_file_to_document(file_path_list_all: List[str]) -> List[Document]:\n",
    "    documents = []\n",
    "    for file_path in file_path_list_all:\n",
    "        # 确保路径是文件而不是目录\n",
    "        if os.path.isfile(file_path):\n",
    "            document = Document(page_content=\"\", metadata={})\n",
    "            filename, extension = os.path.splitext(file_path)\n",
    "            extension = extension.lstrip(\".\")\n",
    "\n",
    "            with open(file_path, \"r\", encoding='utf-8', errors=\"ignore\") as f:\n",
    "                if extension == \"json\":\n",
    "                    data = json.load(f)\n",
    "                else:\n",
    "                    data = [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "                title = data.get(\"title\", \"\").strip()\n",
    "                time = data.get('time', \"\")\n",
    "                infosource = data.get('infosource', \"\")\n",
    "                metadata = {\n",
    "                    \"title\": title,\n",
    "                    \"time\": time[0] if time else \"\",\n",
    "                    \"infosource\": infosource\n",
    "                }\n",
    "\n",
    "                context = data.get(\"context\", '')\n",
    "                context_text = \"\\n\".join(context)\n",
    "                context_text = re.sub(r'\\n+', '', context_text)\n",
    "\n",
    "                document.page_content = context_text\n",
    "                document.metadata = metadata\n",
    "                documents.append(document)\n",
    "        else:\n",
    "            print(f\"Skipping directory: {file_path}\")\n",
    "    return documents\n",
    "\n",
    "\n",
    "# 文本分割\n",
    "def split_text(file_path_list_all):\n",
    "    docs_list = parse_file_to_document(file_path_list_all)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=200)\n",
    "    # chunk_size每多少个文本切分一次；chunk_overlap重叠部分是多少个字符\n",
    "    splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "    return splits\n",
    "    # 下面将切分结果进行展示：splits 是一个列表，其中每个元素也是一个列表，表示一个文档的分割结果\n",
    "    # for doc_index, doc_splits in enumerate(splits):\n",
    "    #     print(f\"Document {doc_index + 1}:\")  # 显示文档编号\n",
    "    #     for split_index, split_text in enumerate(doc_splits):\n",
    "    #         print(f\"  Split {split_index + 1}: {split_text[:50]}...\")  # 打印每个分段的前50个字符\n",
    "    #     print(\"\\n\" + \"-\" * 60 + \"\\n\")  # 在每个文档之间加入分隔线，增加可读性\n",
    "\n",
    "\n",
    "# 数据库创建\n",
    "def create_my_db(split_docs):\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=embedding_model_path)\n",
    "\n",
    "    \n",
    "    # 定义持久化路径\n",
    "    # 加载数据库\n",
    "    vectordb = Milvus.from_documents(\n",
    "        documents=split_docs,\n",
    "        embedding=embeddings,\n",
    "        connection_args={\n",
    "        \"uri\": persist_directory + \"/milvus_demo.db\",\n",
    "    },\n",
    "        # connection_args={\"host\": \"127.0.0.1\", \"port\": \"6006\"}\n",
    "        drop_old=True,\n",
    "    )\n",
    "    # 将加载的向量数据库持久化到磁盘上\n",
    "    # vectordb.persist()\n",
    "\n",
    "# 数据库创建\n",
    "def create_mix_db(split_docs):\n",
    "    dense_embeddings = HuggingFaceEmbeddings(model_name=embedding_model_path)\n",
    "    # sparse_embeddings = BM25SparseEmbedding(corpus=split_docs)\n",
    "    \n",
    "    data_context = [file.page_content for file in split_docs]\n",
    "    sparse_embeddings = BM25SparseEmbedding(corpus=data_context)\n",
    "    \n",
    "    \n",
    "    # 定义持久化路径\n",
    "    # 加载数据库\n",
    "    vectordb = Milvus.from_documents(\n",
    "        documents=split_docs,\n",
    "        embedding=[dense_embeddings, sparse_embeddings],\n",
    "        connection_args={\n",
    "        \"uri\": persist_directory + \"/milvus_mix_demo.db\",\n",
    "    },\n",
    "        # connection_args={\"host\": \"127.0.0.1\", \"port\": \"6006\"}\n",
    "        vector_field=[\"dense_vector\", \"sparse_vector\"],  # 指定向量字段名\n",
    "        # drop_old=True,\n",
    "        auto_id=True\n",
    "        \n",
    "    )\n",
    "    # 将加载的向量数据库持久化到磁盘上\n",
    "    # vectordb.persist()\n",
    "\n",
    "\n",
    "def add_new_data_to_db(new_split_docs, persist_directory):\n",
    "    # 加载已有的数据库\n",
    "    vectordb = Chroma(\n",
    "        persist_directory=persist_directory,\n",
    "        embedding=HuggingFaceEmbeddings(model_name=embedding_model_path)\n",
    "    )\n",
    "    # 将新的数据添加到数据库中\n",
    "    vectordb.add_documents(new_split_docs)\n",
    "    # 将更新后的数据库持久化到磁盘上\n",
    "    vectordb.persist()\n",
    "\n",
    "\n",
    "# 测试生成的document是否正确\n",
    "# def t0():\n",
    "#     file_path_list_all = get_file_name_form_folder(json_folder_path)\n",
    "#     documents = parse_file_to_document(file_path_list_all)\n",
    "#     print(documents)\n",
    "#     for i, document in enumerate(documents):\n",
    "#         print(f\"{i+1}：document: {document}\")\n",
    "\n",
    "\n",
    "# # 运行主函数\n",
    "# if __name__ == \"__main__\":\n",
    "file_path_list_all = get_file_name_form_folder(json_folder_path)\n",
    "split_texts = split_text(file_path_list_all)\n",
    "# create_my_db(split_texts)  # 创建向量数据库，并传入数据\n",
    "create_mix_db(split_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ed0932d-929d-4d06-86a1-425b61240776",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_context = [file.page_content for file in split_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2882f02-4f7a-4b5d-9fe1-a498977feffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('BM25初始化数据.xlsx', 'a', newline='', encoding='utf-8') as f:\n",
    "    for data_ in data_context:\n",
    "        f.write(data_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8ded951-9357-43d2-9747-3108e2d1df95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_context = pd.DataFrame(data_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c9ff7c1-b6ef-48dc-85d2-7e091b1abde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_context.to_excel('BM25初始化数据.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dafa5a2-201e-455f-9324-adecbbe94cf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
