{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ad5bf67-3e97-4ad6-801a-ef63e5cd6770",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from typing import Optional, List, Any\n",
    "import torch\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig, TextIteratorStreamer, AutoProcessor\n",
    "# from modelscope import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.outputs.generation import GenerationChunk\n",
    "from torch import device\n",
    "import time\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "\n",
    "\n",
    "from threading import Thread\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cb65caa-1919-4599-bc5d-9023aec1081c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 构建模型\n",
    "class QianWenLLM(LLM):\n",
    "    # 基于本地的QianWen7B-Chat模型自定义LLM类\n",
    "    tokenizer: AutoTokenizer = None\n",
    "    model: AutoModelForCausalLM = None\n",
    "    processor: AutoProcessor = None\n",
    "    \n",
    "    def __init__(self, model_dir: str):\n",
    "        # 从本地加载模型\n",
    "        super().__init__()\n",
    "        print('正从本地加载模型。。。。。')\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            pretrained_model_name_or_path=model_dir,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            pretrained_model_name_or_path=model_dir,\n",
    "            device_map='auto',\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            # temperature=0.1\n",
    "            )\n",
    "        self.model = self.model.eval()\n",
    "        self.model.generation_config = GenerationConfig.from_pretrained(\n",
    "            model_dir,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        # 可指定不同的生成长度、top_p等相关超参\n",
    "        self.processor = AutoProcessor.from_pretrained(model_dir)\n",
    "        print('模型加载完成！')\n",
    "\n",
    "    def _call(self, prompt: str,\n",
    "              stop: Optional[List[str]] = None,\n",
    "              run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "              **kwargs: Any):\n",
    "        # print('_call函数内查看prompt', prompt)\n",
    "        response, history = self.model.chat(self.tokenizer, prompt, history=[])\n",
    "        # response, history = self.model.generate(self.tokenizer, prompt, history=[])\n",
    "        return response\n",
    "        \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"QwenLM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d379e00-210e-4884-b465-f1a772301df5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_model():\n",
    "    # 初始化模型\n",
    "    model_cache_path = r'autodl-tmp/Qwen/Qwen-7B-Chat'\n",
    "    # model_cache_path = r'autodl-tmp/Qwen/Qwen2.5-7B-Instruct'\n",
    "    llm = QianWenLLM(model_dir=model_cache_path)\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "623079af-7c08-444c-9478-c31b60f0aff6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正从本地加载模型。。。。。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to \"AutoModelForCausalLM.from_pretrained\".\n",
      "Try importing flash-attention for faster inference...\n",
      "Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\n",
      "Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n",
      "Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2199d01bc0da421e88b3a3c779e981d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "The repository for autodl-tmp/Qwen/Qwen-7B-Chat contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/autodl-tmp/Qwen/Qwen-7B-Chat.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n",
      "The repository for autodl-tmp/Qwen/Qwen-7B-Chat contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/autodl-tmp/Qwen/Qwen-7B-Chat.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型加载完成！\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'非常抱歉，我无法提供您需要的内容。我是一个人工智能语言模型，我的知识库主要集中在公共领域的常见问题和一般性话题上。如果您有关于其他主题的问题，请随时告诉我，axe'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = init_model()\n",
    "question = '请展示《上海联合产权交易所有限公司物权交易操作指引》的具体内容'\n",
    "template = f\"\"\"\n",
    "            回答输入的问题，如果你不知道答案，就说你不知道，不要试图编造答案。\n",
    "            输入的问题：{question}\n",
    "            有用的回答：\n",
    "\"\"\"\n",
    "llm.invoke(template.format(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa35931c-ac49-44fd-aa81-a3a782af46a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentence = [\n",
    "            '你好',\n",
    "            '你是谁？',\n",
    "            '上海联合产权交易所有限公司物权交易操作指引(沪联产交〔2020〕26号)',\n",
    "            '周杰伦是谁？',\n",
    "            '产权交易争议调解',\n",
    "            '2023年7月，中国航天科技集团公司五院研制的长征十二号运载火箭在酒泉卫星发射中心成功发射，将一颗北斗导航卫星送入预定轨道。',\n",
    "            '周杰伦的歌曲《稻香》是谁写的？',\n",
    "            '项目名称为ABC，中标金额为500万元，供应商为XYZ公司。']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3966af8-c640-48ba-a724-96b0e6e0d008",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = f\"\"\"\n",
    "        你是一名实体提取和意图识别分类领域专家，请严格遵循以下任务和工作流程的指示输出结果。\n",
    "        \n",
    "        任务：\n",
    "        1-判断输入文本是否存在实体。\n",
    "        2-抽取输入文本所有实体。\n",
    "        3-根据实体与给出的意图标签进行判定该输入文本的意图。\n",
    "        \n",
    "        意图标签：政策知识，日常知识，招中标知识\n",
    "        \n",
    "        工作流程：\n",
    "        1.-先判断是否存在实体，不存在实体则直接根据不存在实体输出格式输出，存在实体则继续以下工作流程，并通过存在实体输出格式输出。\n",
    "        2-实体提取：请从输入的文本中提取出所有实体。\n",
    "        3-意图分类：请根据第2点提取的实体以及意图标签，进行意图识别并分类输入文本。\n",
    "        \n",
    "        输入文本：{question}\n",
    "        不存在实体输出格式：\n",
    "            实体提取:[]\n",
    "            意图分类:日常知识\n",
    "        存在实体输出格式：\n",
    "            实体提取：[实体1, 实体2, ...]\n",
    "            意图分类：意图标签\n",
    "\n",
    "        有用的回答：\n",
    "       \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
