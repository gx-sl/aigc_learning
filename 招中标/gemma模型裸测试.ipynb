{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5634e2be-51c3-4d89-b4fc-6990e601610e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-03 17:53:06,097 - modelscope - INFO - PyTorch version 2.1.2+cu118 Found.\n",
      "2024-11-03 17:53:06,098 - modelscope - INFO - Loading ast index from /root/.cache/modelscope/ast_indexer\n",
      "2024-11-03 17:53:06,132 - modelscope - INFO - Loading done! Current index file version is 1.9.5, with md5 4ac51ac13967d1218b2f87d3c7f91cc4 and a total number of 945 components indexed\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from typing import Optional, List, Any\n",
    "import torch\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from modelscope import AutoTokenizer, AutoModelForCausalLM\n",
    "# from modelscope import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.outputs.generation import GenerationChunk\n",
    "from torch import device\n",
    "import time\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain.chains.history_aware_retriever import create_history_aware_retriever\n",
    "\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain_core.runnables import RunnableWithMessageHistory\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "from threading import Thread\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dd9fb13-e15f-4163-b5c9-b33341b50017",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2cf1966-ec1f-4d05-841d-8d8ce222232e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "957e5ed9961f4fc29b2916a044be4b5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>你是谁？\n",
      "\n",
      "**我是一个大型语言模型，由 Google 训练。**\n",
      "\n",
      "我被设计来用文本生成式的方式进行交流，我无法访问实时信息或进行谷歌搜索。<end_of_turn>\n",
      "<eos>\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained( r'autodl-tmp/LLM-Research/gemma-2-9b-it')\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "     r'autodl-tmp/LLM-Research/gemma-2-9b-it',\n",
    "    # device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ").to(device)\n",
    "input_text = \"你是谁？\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**input_ids)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b35264b-cea3-42b6-a6bd-b46c85c04d82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     2,  51641, 237144, 235544,    109,    688, 235509,  37365,  66240,\n",
       "          48982,  43936, 235365, 236102,   6238, 235248,  50710, 235362,    688,\n",
       "            109, 235509, 235936,  13586, 235547, 235522,  77053,  40970, 235830,\n",
       "          60891,  14379,  46645, 235365, 235509,  30876,  48692, 128086,  12620,\n",
       "         236132,  14379, 159620,  43944, 235362,    107,    108,      1]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b508ae2-a074-4fe1-a156-ee7bb5e87869",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
