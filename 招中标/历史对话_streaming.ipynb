{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ad5bf67-3e97-4ad6-801a-ef63e5cd6770",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from typing import Optional, List, Any\n",
    "import torch\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig, TextIteratorStreamer, AutoProcessor\n",
    "# from modelscope import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.outputs.generation import GenerationChunk\n",
    "from torch import device\n",
    "import time\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain.chains.history_aware_retriever import create_history_aware_retriever\n",
    "\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain_core.runnables import RunnableWithMessageHistory\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "from threading import Thread\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7eb6f49-fd63-4004-bfc1-25221b2d97b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cb65caa-1919-4599-bc5d-9023aec1081c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 构建模型\n",
    "class QianWenLLM(LLM):\n",
    "    # 基于本地的QianWen7B-Chat模型自定义LLM类\n",
    "    tokenizer: AutoTokenizer = None\n",
    "    model: AutoModelForCausalLM = None\n",
    "    processor: AutoProcessor = None\n",
    "    \n",
    "    def __init__(self, model_dir: str):\n",
    "        # 从本地加载模型\n",
    "        super().__init__()\n",
    "        print('正从本地加载模型。。。。。')\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            pretrained_model_name_or_path=model_dir,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            pretrained_model_name_or_path=model_dir,\n",
    "            device_map='auto',\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch.bfloat16\n",
    "            )\n",
    "        self.model = self.model.eval()\n",
    "        self.model.generation_config = GenerationConfig.from_pretrained(\n",
    "            model_dir,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        # 可指定不同的生成长度、top_p等相关超参\n",
    "        self.processor = AutoProcessor.from_pretrained(model_dir)\n",
    "        print('模型加载完成！')\n",
    "\n",
    "    def _call(self, prompt: str,\n",
    "              stop: Optional[List[str]] = None,\n",
    "              run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "              **kwargs: Any):\n",
    "        # print('_call函数内查看prompt', prompt)\n",
    "        response, history = self.model.chat(self.tokenizer, prompt, history=[])\n",
    "        return response\n",
    "        \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"QwenLM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14072543-030e-4ed8-935d-0a5ab13df102",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 构建改写问题proompt\n",
    "def contextualize_question_prompt():\n",
    "    system_prompt = \"\"\"\\\n",
    "    请根据聊天历史和最后用户的问题，改写用户最终剔除的问题。\n",
    "    你只需要改写用户最终的问题，请不要回答问题。\n",
    "    没有聊天历史则将用户问题直接返回，有聊天历史则改写。\n",
    "    \"\"\"\n",
    "    contextualize_question_prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        ('system', system_prompt),\n",
    "        MessagesPlaceholder('chat_history'),\n",
    "        ('human', '{input}')\n",
    "    ]\n",
    "    )\n",
    "    return contextualize_question_prompt\n",
    "\n",
    "# 构建正常问答prompt\n",
    "def answer_prompt():\n",
    "    system_prompt = \"\"\"\\\n",
    "    使用上下文来回答最后的问题。如果你不知道答案，就说你不知道，不要试图编造答案。\n",
    "    {context}\n",
    "    \"\"\"\n",
    "    # 问题: {question}\n",
    "    # 有用的回答:\n",
    "    \n",
    "    qa_prompt = ChatPromptTemplate(\n",
    "    [\n",
    "    ('system', system_prompt),\n",
    "        MessagesPlaceholder('chat_history'),\n",
    "        ('human', '{input}')\n",
    "    ]\n",
    "    )\n",
    "    return qa_prompt\n",
    "\n",
    "# 构造存储函数\n",
    "def get_session_history(session_id: str):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b9f8397-d128-4dbc-a209-746e34a9828b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def qa_history_chain():\n",
    "    \"\"\"\n",
    "    构建问答链\n",
    "    :param persist_directory: 知识库本地保存路径，这里初始化政策信息知识库\n",
    "    :return: 返回调用LLM回答\n",
    "    \"\"\"\n",
    "    persist_directory = r'vectordb/chroma'\n",
    "    embeddings_model_cache_path = r'autodl-tmp/embedding_model/Ceceliachenen/paraphrase-multilingual-MiniLM-L12-v2'\n",
    "    # 加载词向量模型\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=embeddings_model_cache_path)\n",
    "    # 加载缓存知识库\n",
    "    vectordb = Chroma(\n",
    "        persist_directory=persist_directory,\n",
    "        embedding_function=embeddings,\n",
    "    )\n",
    "    # 初始化模型\n",
    "    model_cache_path = r'autodl-tmp/Qwen/Qwen-7B-Chat'\n",
    "    llm = QianWenLLM(model_dir=model_cache_path)\n",
    "\n",
    "    \n",
    "    question_prompt = contextualize_question_prompt()\n",
    "    history_aware_retriever = create_history_aware_retriever(llm, vectordb.as_retriever(), question_prompt)\n",
    "    \n",
    "    qa_prompt = answer_prompt()\n",
    "    qa_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "    rag_chain = create_retrieval_chain(history_aware_retriever, qa_chain)\n",
    "    \n",
    "    # 包装\n",
    "    conversational_rag_chain = RunnableWithMessageHistory(\n",
    "        runnable=rag_chain,\n",
    "        get_session_history=get_session_history,\n",
    "        input_messages_key='input',\n",
    "        history_messages_key='chat_history',\n",
    "        output_messages_key='answer'\n",
    "    )\n",
    "\n",
    "    return conversational_rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91cdf3ef-4397-420a-9a24-b2133f27f803",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "store= {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "333bcc19-cc3e-42c7-8655-8fa8630b372c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 链接前面的函数\n",
    "class Model_center():\n",
    "    \"\"\"\n",
    "      存储问答 Chain 的对象\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ):\n",
    "        print('初始化知识库问答链。。。。')\n",
    "        self.qa_chain = qa_history_chain()\n",
    "\n",
    "    def qa_chain_self_answer(self, question: str, chat_history: list = []):\n",
    "        print('调用问答链')\n",
    "        # print('打印用户问题', question)\n",
    "        if question == None or len(question) < 1:\n",
    "            print('问答为空。。。。')\n",
    "            return '', chat_history\n",
    "        # try:\n",
    "        print('调用检索问答链。。。。')\n",
    "        # 结果调用下流式输出\n",
    "        # response = self.qa_chain.invoke({'query': question})['result']\n",
    "        \n",
    "        # 检索问答链+历史聊天组件\n",
    "        response = self.qa_chain.invoke(\n",
    "            {'input': question},\n",
    "            config={'configurable':{'session_id':'test123'}}\n",
    "        )['answer']\n",
    "\n",
    "        chat_history.append([question, response])\n",
    "        # print(chat_history)\n",
    "        return '', chat_history\n",
    "                \n",
    "        # except Exception as e:\n",
    "        #     print('问答链报错', e)\n",
    "        #     return e, chat_history\n",
    "\n",
    "    def clear_history(self):\n",
    "        self.qa_chain.clear_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7abcb68b-dbaa-4139-941b-37014e7d7c2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化知识库问答链。。。。\n",
      "正从本地加载模型。。。。。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to \"AutoModelForCausalLM.from_pretrained\".\n",
      "Try importing flash-attention for faster inference...\n",
      "Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\n",
      "Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n",
      "Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d3daceba2cc48f8a6ff150f29dccc92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "The repository for autodl-tmp/Qwen/Qwen-7B-Chat contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/autodl-tmp/Qwen/Qwen-7B-Chat.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n",
      "The repository for autodl-tmp/Qwen/Qwen-7B-Chat contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/autodl-tmp/Qwen/Qwen-7B-Chat.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型加载完成！\n",
      "进度1\n",
      "进度2\n",
      "Running on local URL:  http://127.0.0.1:6006\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:6006/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPORTANT: You are using gradio version 3.50.2, however version 4.44.1 is available, please upgrade.\n",
      "--------\n",
      "调用问答链\n",
      "调用检索问答链。。。。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "调用问答链\n",
      "调用检索问答链。。。。\n",
      "调用问答链\n",
      "调用检索问答链。。。。\n",
      "调用问答链\n",
      "调用检索问答链。。。。\n"
     ]
    }
   ],
   "source": [
    "model_center = Model_center()\n",
    "def update_chatbot(question, chat_history):\n",
    "    for char in model_center.qa_chain_self_answer(question, chat_history):\n",
    "        gr.update(value=chat_history)\n",
    "        chat_history.append((question, char))\n",
    "    return chat_history\n",
    "# def demo():\n",
    "block = gr.Blocks()\n",
    "with block as demo:\n",
    "    with gr.Row(equal_height=True):  # 水平排列子组件\n",
    "        with gr.Column(scale=15):  # 垂直排列子组件\n",
    "            gr.Markdown(\"\"\"<h1><center>QwenLM7B-Chat</center></h1><center>科大讯飞实践-招中标政策智能问答助手</center>\"\"\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=4):\n",
    "            # 创建聊天界面的组件。height=450 参数设置了聊天界面的高度为 450 像素。\n",
    "            # show_copy_button=True参数表示在聊天界面中显示一个复制按钮，允许用户复制聊天内容\n",
    "            chatbot = gr.Chatbot(height=450, show_copy_button=True)\n",
    "            # 创建一个文本框组件，用于输入 prompts。\n",
    "            msg = gr.Textbox(label='Prompt/问题')\n",
    "\n",
    "            with gr.Row():\n",
    "                # 创建提交按钮\n",
    "                db_wo_his_btn = gr.Button('Chat')\n",
    "            with gr.Row():\n",
    "                # 创建一个清除按钮，用于清除聊天机器人组件的内容。\n",
    "                clear_btn = gr.ClearButton(components=[chatbot], value='Clear console')\n",
    "\n",
    "        # 设置按钮的点击事件。当点击时，调用上面定义的 qa_chain_self_answer 函数，并传入用户的消息和聊天历史记录，然后更新文本框和聊天机器人组件。\n",
    "        print('进度1')\n",
    "        # 设置流式输出\n",
    "        def bot(question, history):\n",
    "            # print('bot_question',question)\n",
    "            # print('bot_history',history)\n",
    "            curr, response = model_center.qa_chain_self_answer(question, history)\n",
    "            # print('response', response)\n",
    "            # print('curr', curr)\n",
    "            history = response\n",
    "            bot_message = history[-1][1]\n",
    "            # print('bot_message', bot_message)\n",
    "            history[-1][1] = ''\n",
    "            for character in bot_message:\n",
    "                history[-1][1] += character\n",
    "                # print(f'累计中：{history}')\n",
    "                time.sleep(0.1)\n",
    "                yield '', history\n",
    "        \n",
    "        db_wo_his_btn.click(bot, inputs=[msg, chatbot], outputs=[msg, chatbot])\n",
    "\n",
    "        print('进度2')\n",
    "        # 点击后清空后端存储的聊天记录\n",
    "        clear_btn.click(model_center.clear_history)\n",
    "\n",
    "    # 填写注意事项\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        提醒：<br>\n",
    "        1. 初始化数据库实践可能较长，请耐心等待。\n",
    "        2. 使用中如果出现异常，将会在文本输入框进行展示，请不要惊慌。 <br>\n",
    "        \"\"\"\n",
    "    )\n",
    "# gr.close_all()\n",
    "# 直接启动\n",
    "demo.queue()\n",
    "demo.launch(server_name='127.0.0.1', server_port=6006)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa35931c-ac49-44fd-aa81-a3a782af46a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test123': InMemoryChatMessageHistory(messages=[HumanMessage(content='你好'), AIMessage(content='您好！有什么我能帮助您的吗？'), HumanMessage(content='王二有个哥哥叫王大'), AIMessage(content='这是一个假设性的场景，没有实际背景或上下文可供参考。请提供更多信息以便我可以更好地回答您的问题。'), HumanMessage(content='王二的哥哥叫什么名字'), AIMessage(content='王大的名字。'), HumanMessage(content='王二的哥哥叫什么'), AIMessage(content='王大的名字是王二的哥哥。')])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3966af8-c640-48ba-a724-96b0e6e0d008",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
