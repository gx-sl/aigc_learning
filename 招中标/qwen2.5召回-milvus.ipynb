{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "906ce176-935f-44d0-b65f-480deacfed19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from typing import Optional, List, Any\n",
    "import torch\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain_milvus import Milvus\n",
    "from langchain_milvus.utils.sparse import BM25SparseEmbedding\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig, TextIteratorStreamer, AutoProcessor\n",
    "# from modelscope import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.outputs.generation import GenerationChunk\n",
    "from torch import device\n",
    "import time\n",
    "from langchain import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain.chains.history_aware_retriever import create_history_aware_retriever\n",
    "\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain_core.runnables import RunnableWithMessageHistory\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# from threading import Thread\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c892838-f4a7-48b1-99d6-7bedb90b8120",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bb71bc6-feb2-4935-b026-95862673728f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 构建模型\n",
    "class QianWenLLM(LLM):\n",
    "    # 基于本地的QianWen7B-Chat模型自定义LLM类\n",
    "    tokenizer: AutoTokenizer = None\n",
    "    model: AutoModelForCausalLM = None\n",
    "    processor: AutoProcessor = None\n",
    "    \n",
    "    def __init__(self, model_dir: str):\n",
    "        # 从本地加载模型\n",
    "        super().__init__()\n",
    "        print('正从本地加载模型。。。。。')\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            pretrained_model_name_or_path=model_dir,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            pretrained_model_name_or_path=model_dir,\n",
    "            # device_map='auto',\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            # temperature=0\n",
    "            ).to(device)\n",
    "        self.model = self.model.eval()\n",
    "        self.model.generation_config = GenerationConfig.from_pretrained(\n",
    "            model_dir,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        # 可指定不同的生成长度、top_p等相关超参\n",
    "        self.processor = AutoProcessor.from_pretrained(model_dir)\n",
    "        print('模型加载完成！')\n",
    "             \n",
    "    def _call(self, infor,\n",
    "              stop: Optional[List[str]] = None,\n",
    "              run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "              **kwargs: Any):\n",
    "        # response, history = self.model.chat(self.tokenizer, prompt, history=[])\n",
    "        # print('infor:', infor)\n",
    "        # system_info =  infor.split('说“谢谢你的提问！”。')[-1].split('问题：')[0].strip()\n",
    "        # human_info =  infor.split('问题：')[-1].strip()\n",
    "        # time.sleep(1000)\n",
    "        messages = [\n",
    "                    # {'role':'system', 'content': system_info},\n",
    "                    {'role':'user', 'content': infor}\n",
    "                    ]\n",
    "        # print('打印messages内容：', messages)\n",
    "        # print('打印messages类型：', type(messages))\n",
    "        text = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        model_inputs = self.tokenizer([text], return_tensors='pt').to(device)\n",
    "        generated_ids = self.model.generate(\n",
    "            **model_inputs,\n",
    "            max_new_tokens=1024\n",
    "        )\n",
    "        generated_ids = [\n",
    "            output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "        response = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "       \n",
    "        return response\n",
    "        \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"QwenLM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a0aa4c1-af87-4d29-8478-9875ca3278a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_db():\n",
    "    # persist_directory = r'vectordb/chroma/m3e-base'\n",
    "    persist_directory = r'vectordb/milvus/bag-large/milvus_demo.db'\n",
    "    # embeddings_model_cache_path = r'autodl-tmp/embedding_model/AI-ModelScope/m3e-base'\n",
    "    \n",
    "    embeddings_model_cache_path = r'autodl-tmp/embedding_model/BAAI/bge-large-zh-v1___5'\n",
    "    # 加载词向量模型\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=embeddings_model_cache_path)\n",
    "    \n",
    "    # 加载缓存知识库\n",
    "    vectordb = Milvus(\n",
    "        connection_args={'uri': persist_directory},\n",
    "        embedding_function=embeddings,\n",
    "    )\n",
    "    return vectordb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ef66d60-7bcc-4ca2-866d-d1f6ecb4c9e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_model(vectordb):\n",
    "    # 初始化模型\n",
    "    # model_cache_path = r'autodl-tmp/Qwen/Qwen-7B-Chat'\n",
    "    model_cache_path = r'autodl-tmp/Qwen/Qwen2.5-7B-Instruct'\n",
    "    llm = QianWenLLM(model_dir=model_cache_path)\n",
    "    template = \"\"\"\n",
    "        使用上下文来回答最后的问题。如果你不知道答案，就说你不知道，不要试图编造答案。总是在回答的最后说“谢谢你的提问！”。\n",
    "        {context}\n",
    "        问题：{question}\n",
    "        \"\"\"\n",
    "    QA_CHAIN_PROMPT = PromptTemplate(input_variables=['context', 'question'], template=template)\n",
    "\n",
    "    # 构造检索问答链\n",
    "    qa_chain_ = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        retriever=vectordb.as_retriever(\n",
    "            search_type=\"mmr\", \n",
    "            search_kwargs={\"k\": 10}\n",
    "        # search_kwargs={\"k\": 10, \"fetch_k\": 50, 'index_type': \"mmr\", 'metric_type':'L2'}\n",
    "        ),\n",
    "        # retriever = vectordb.max_marginal_relevance_search_by_vector(limit=10)\n",
    "        return_source_documents=True,\n",
    "        chain_type_kwargs={'prompt': QA_CHAIN_PROMPT},\n",
    "        # chain_type='stuff'\n",
    "    )\n",
    "    return qa_chain_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f29765f-5f17-42cb-bf93-2a02b3d6de9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init2_model(vectordb):\n",
    "    # 初始化模型\n",
    "    # model_cache_path = r'autodl-tmp/Qwen/Qwen-7B-Chat'\n",
    "    model_cache_path = r'autodl-tmp/Qwen/Qwen2.5-7B-Instruct'\n",
    "    llm = QianWenLLM(model_dir=model_cache_path)\n",
    "    # template = \"\"\"\n",
    "    #     使用上下文来回答最后的问题。如果你不知道答案，就说你不知道，不要试图编造答案。总是在回答的最后说“谢谢你的提问！”。\n",
    "    #     上下文：{context}\n",
    "    #     问题：{question}\n",
    "    #     回答：\n",
    "    #     \"\"\"\n",
    "    template = \"\"\"\n",
    "    分析输入的上下文，段落之间利用空行进行分隔，对每一个段落与问题进行相似度匹配，找出与问题相似度高的段落文本，并直接将文本输出。\n",
    "    上下文：{context}\n",
    "    问题：{question}\n",
    "    请不要作答，直接输出匹配的文本。\n",
    "    \"\"\"\n",
    "    QA_CHAIN_PROMPT = PromptTemplate(input_variables=['context', 'question'], template=template)\n",
    "\n",
    "    qa_chain_ = LLMChain(llm=llm, prompt=QA_CHAIN_PROMPT)\n",
    "\n",
    "    return qa_chain_, llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42bda1b2-6618-4177-8b21-fa63ffac64b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正从本地加载模型。。。。。\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e519a4f54ec4fe0acd484d4355bd247",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型加载完成！\n"
     ]
    }
   ],
   "source": [
    "# 加载数据\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "# import re\n",
    "# from tqdm import tqdm\n",
    "# 初始化模型\n",
    "vectordb = init_db()\n",
    "model = init_model(vectordb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03dcee58-1ef0-4876-b877-fb4265bb4977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f95021d7d5ef41a6a145e4bef9b80202",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    }
   ],
   "source": [
    "datas_ = pd.read_excel('QA_test.xlsx') \n",
    "# datas_ = datas[datas['问题分类'] == '政策内容']Question\tAnswer\tText\tTitle\n",
    "\n",
    "questions = datas_['Question']\n",
    "titles = datas_['Title']\n",
    "# labels = datas_['问题分类']\n",
    "# bodys = datas_['提取的实体']\n",
    "file = 'milvus召回测试-L2.csv'\n",
    "for question, title in tqdm(zip(questions, titles), ncols=len(questions)):\n",
    "# question = '意向受让方在登记受让意向时需要满足哪些条件？'\n",
    "\n",
    "    response = model.invoke({'query': question})\n",
    "    source_title = [doc.metadata['title'] for doc in response['source_documents']]\n",
    "    # source_title = [file_.strip() for file_ in source_title]\n",
    "    # print('问题：', question)\n",
    "    # print('回答：', response['result'])\n",
    "    # print('来源文档：', source_title)\n",
    "    for tt in range(len(source_title)):\n",
    "        if title == source_title[tt]:\n",
    "            calling_ = 'True'\n",
    "            calling_loc = tt\n",
    "            break\n",
    "        else:\n",
    "            calling_ = 'False'\n",
    "            calling_loc = '无'\n",
    "        \n",
    "   \n",
    "    data_ = [question, source_title, title, calling_, calling_loc]\n",
    "    # print(data_)\n",
    "    with open(file, 'a', newline='', encoding='utf-8') as f:\n",
    "        fw = csv.writer(f)\n",
    "        if not os.path.getsize(file):\n",
    "            header = ['问题', '召回文档标题', '真实标题', '正确召回', '召回位置']\n",
    "            fw.writerow(header)\n",
    "        fw.writerow(data_)\n",
    "\n",
    "# print('文档内容', response['source_documents'])\n",
    "# time.sleep(1000)\n",
    "# print('问题：', question)\n",
    "# print('回答：', response['result'])\n",
    "# print('来源文档：', response['source_documents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20252d3f-b442-4c86-9b32-41c4a4a60e8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29745463-1518-40c4-b8da-274fe5bf6ce3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datas = pd.read_csv('实体提取_改进.csv') \n",
    "datas_ = datas[datas['问题分类'] == '政策内容']\n",
    "questions = datas_['问题']\n",
    "labels = datas_['问题分类']\n",
    "bodys = datas_['提取的实体']\n",
    "# question = '意向受让方,登记受让意向,条件'\n",
    "# 测试用实体来召回文本，并用召回的文本和问题回答\n",
    "for question, body_ in tqdm(zip(questions, bodys), ncols=len(questions)):\n",
    "    # 这里可能要将召回的问答链进行拆分，分两步\n",
    "    question = '开标时，投标文件是如何被处理的？'\n",
    "    # body_ = '上海市规划和自然资源局关于印发《国有建设用地使用权招标拍卖挂牌出让投标竞买外汇保证金账户管理制度》的通知'\n",
    "    # body_ = '投标人,竞买人,违约,保证金,处理'\n",
    "    body_ = '开标，投标文件'\n",
    "    call_data = vectordb.as_retriever(\n",
    "        search_type=\"mmr\", \n",
    "        search_kwargs={\"k\": 10, \"fetch_k\": 20}).get_relevant_documents(body_ )\n",
    "    # print(call_data)\n",
    "    print([doc.metadata['title'] for doc in call_data])\n",
    "    page_contents = ''\n",
    "    for doc in call_data:\n",
    "        page_contents += doc.page_content.strip() + '\\n\\n'\n",
    "\n",
    "    # 提取文本摘要呢？\n",
    "    response = model.run({'context': page_contents, 'question': question})\n",
    "    print('匹配response:', response)\n",
    "    print('++++'*100)\n",
    "    time.sleep(1000)\n",
    "    # print('问题：', question)\n",
    "    # print('回答：', response['result'])\n",
    "    # print('来源文档：', response['source_documents'])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "42db3d3f-9c23-4acc-902d-ba92586ca389",
   "metadata": {},
   "source": [
    "1. 当投标方或竞拍者存在违约行为时，保证金应该怎样处理？\n",
    "\n",
    "2. 如果投标者或竞买者违反了合同条款，保证金会如何处置？\n",
    "\n",
    "3. 在投标者或竞买者违约的情况下，保证金应该如何处理？\n",
    "\n",
    "4. 对于违约的投标方或竞买人，其缴纳的保证金该如何处理？\n",
    "\n",
    "5. 投标人或竞买人若出现违约，保证金应如何进行处理？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7a82a4-8502-464a-b9d9-a47c4f6b2533",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15db80da-dfa6-45cf-bed3-ad44872cf80c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1370ae14-a56f-46b4-8850-502a7da74572",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03928d99-8b26-4328-8da3-62955f57fed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "第一阶段召回改善方式\n",
    "0、扩大召回数目；\n",
    "1、提取问题关键词、实体， 每个实体随意组合以及单个实体进行多次召回；\n",
    "2、对问题进行重新生成多种说法，召回多次；（备选，因为我们在做意图识别时会进行实体提取，如果提取成功率较高的话，这一步就可以省略，没必要生成多个问题）；\n",
    "3、rerank排序召回，考虑第1、2点的多次召回重复较多的文档title作为一个权重排序；\n",
    "'''"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0dad9d66-90e6-4b62-b68e-6d71f62c7c68",
   "metadata": {},
   "source": [
    "验证0、1、2对有些问题的源文档片段还是无法召回\n",
    "继续扩大返回后，出现原文档，但是文档信息多，还是无法正确回复---直接匹配问题，提取中间及靠前的几个文本会出现正确答案\n",
    "提取的实体，有些动词会造成误导致使召回失误，因此需要多次召回，并且召回后进行排序，并剔除重复值\n",
    "存在召回问题的，经过各种措施召回后，可能排序落后，因此最终的排序应该是最前面几个+中间几个+最后几个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685559af-a93f-41dd-8435-5deb69348507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# persist_directory = r'vectordb/chroma/m3e-base'\n",
    "persist_directory = r'vectordb/milvus/bag-large/milvus_demo.db'\n",
    "# embeddings_model_cache_path = r'autodl-tmp/embedding_model/AI-ModelScope/m3e-base'\n",
    "\n",
    "embeddings_model_cache_path = r'autodl-tmp/embedding_model/BAAI/bge-large-zh-v1___5'\n",
    "# 加载词向量模型\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=embeddings_model_cache_path)\n",
    "\n",
    "# 加载缓存知识库\n",
    "vectordb = Milvus(\n",
    "    connection_args={'uri': persist_directory},\n",
    "    embedding_function=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f70c885d-9104-4295-91c5-bb5819dccc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_title = [doc.metadata['title'] for doc in response['source_documents']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fade929-3b14-4b10-88a2-50726d275e49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['招标公告和公示信息发布管理办法',\n",
       " '铁路工程建设项目招标投标管理办法',\n",
       " '政府采购货物和服务招标投标管理办法',\n",
       " '水运工程建设项目招标投标管理办法',\n",
       " '民航局机场司关于印发《民航专业工程建设项目招标投标管理办法》的通知',\n",
       " '招标公告和公示信息发布管理办法',\n",
       " '工程建设项目施工招标投标办法',\n",
       " '招标公告和公示信息发布管理办法',\n",
       " '财政部关于做好政府采购信息公开工作的通知',\n",
       " '工程建设项目货物招标投标办法']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cd75319-dea2-4935-b68b-8df048312daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "if title in source_title:\n",
    "    print(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2adf345-26c2-43a4-9cd5-1019360e4383",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
