{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ad5bf67-3e97-4ad6-801a-ef63e5cd6770",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from typing import Optional, List, Any\n",
    "import torch\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig, TextIteratorStreamer, AutoProcessor\n",
    "# from modelscope import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.outputs.generation import GenerationChunk\n",
    "from torch import device\n",
    "import time\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "\n",
    "\n",
    "from threading import Thread\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cb65caa-1919-4599-bc5d-9023aec1081c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 构建模型\n",
    "class QianWenLLM(LLM):\n",
    "    # 基于本地的QianWen7B-Chat模型自定义LLM类\n",
    "    tokenizer: AutoTokenizer = None\n",
    "    model: AutoModelForCausalLM = None\n",
    "    processor: AutoProcessor = None\n",
    "    \n",
    "    def __init__(self, model_dir: str):\n",
    "        # 从本地加载模型\n",
    "        super().__init__()\n",
    "        print('正从本地加载模型。。。。。')\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            pretrained_model_name_or_path=model_dir,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            pretrained_model_name_or_path=model_dir,\n",
    "            device_map='auto',\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch.bfloat16\n",
    "            )\n",
    "        self.model = self.model.eval()\n",
    "        self.model.generation_config = GenerationConfig.from_pretrained(\n",
    "            model_dir,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        # 可指定不同的生成长度、top_p等相关超参\n",
    "        self.processor = AutoProcessor.from_pretrained(model_dir)\n",
    "        print('模型加载完成！')\n",
    "\n",
    "    def _call(self, prompt: str,\n",
    "              stop: Optional[List[str]] = None,\n",
    "              run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "              **kwargs: Any):\n",
    "        # print('_call函数内查看prompt', prompt)\n",
    "        response, history = self.model.chat(self.tokenizer, prompt, history=[])\n",
    "        return response\n",
    "        \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"QwenLM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b9f8397-d128-4dbc-a209-746e34a9828b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def qa_chain():\n",
    "    \"\"\"\n",
    "    构建问答链\n",
    "    :param persist_directory: 知识库本地保存路径，这里初始化政策信息知识库\n",
    "    :return: 返回调用LLM回答\n",
    "    \"\"\"\n",
    "    persist_directory = r'vectordb/chroma'\n",
    "    embeddings_model_cache_path = r'autodl-tmp/embedding_model/Ceceliachenen/paraphrase-multilingual-MiniLM-L12-v2'\n",
    "    # 加载词向量模型\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=embeddings_model_cache_path)\n",
    "    # 加载缓存知识库\n",
    "    vectordb = Chroma(\n",
    "        persist_directory=persist_directory,\n",
    "        embedding_function=embeddings,\n",
    "    )\n",
    "    # 初始化模型\n",
    "    model_cache_path = r'autodl-tmp/Qwen/Qwen-7B-Chat'\n",
    "    llm = QianWenLLM(model_dir=model_cache_path)\n",
    "    # 简单测试\n",
    "    # response = llm.invoke(input='你好，你是谁？')\n",
    "    # print(response)\n",
    "    # 构造prompt模板\n",
    "    # template = \"\"\"\n",
    "    # 使用上下文来回答最后的问题。如果你不知道答案，就说你不知道，不要试图编造答案。总是在回答的最后说“谢谢你的提问！”。\n",
    "    # {context}\n",
    "    # 问题：{question}\n",
    "    # 有用的回答：\n",
    "    # \"\"\"\n",
    "    template = \"\"\"\n",
    "    你是一位问题文本实体提取和意图识别专家，将对输入的问题进行实体提取和对提取的实体进行改写生成新文本。\n",
    "    任务：\n",
    "    1.对输入的文本进行实体提取，尽量提取所有实体;\n",
    "    2.将表达意图的几个实体重新生成新文本，新文本应该简洁明了。\n",
    "    3.根据新文本进行问题回复。\n",
    "    4.生成格式：\n",
    "        所有实体：实体1，实体2，实体3...。\n",
    "        意图实体：意图实体1，意图实体2，意图实体3...。\n",
    "        生成问题回复。\n",
    "    问题：{question}\n",
    "    有用的回答：\n",
    "    \"\"\"\n",
    "    \n",
    "    response = llm.invoke(input=template)\n",
    "    print(response)\n",
    "    # 尽量使答案简明扼要。\n",
    "    # 调用 LangChain 的方法来实例化一个 Template 对象，该对象包含了 context 和 question 两个变量，\n",
    "    # 在实际调用时，这两个变量会被检索到的文档片段和用户提问填充\n",
    "    QA_CHAIN_PROMPT = PromptTemplate(input_variables=['context', 'question'], template=template)\n",
    "\n",
    "    # 构造检索问答链\n",
    "    # qa_chain_ = RetrievalQA.from_chain_type(\n",
    "    #     llm=llm,\n",
    "    #     retriever=vectordb.as_retriever(),\n",
    "    #     return_source_documents=True,\n",
    "    #     chain_type_kwargs={'prompt': QA_CHAIN_PROMPT},\n",
    "    #     # chain_type='stuff'\n",
    "    # )\n",
    "    \n",
    "    # 构建历史对话检索问答\n",
    "    memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "    qa_chain_ = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=vectordb.as_retriever(),\n",
    "        memory=memory,\n",
    "        return_source_documents=True,\n",
    "        chain_type_kwargs={'prompt': QA_CHAIN_PROMPT},\n",
    "\n",
    "    )\n",
    "\n",
    "    return qa_chain_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cdf3ef-4397-420a-9a24-b2133f27f803",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "333bcc19-cc3e-42c7-8655-8fa8630b372c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 链接前面的函数\n",
    "class Model_center():\n",
    "    \"\"\"\n",
    "      存储问答 Chain 的对象\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ):\n",
    "        print('初始化知识库问答链。。。。')\n",
    "        self.qa_chain = qa_chain()\n",
    "\n",
    "    def qa_chain_self_answer(self, question: str, chat_history: list = []):\n",
    "        print('调用问答链')\n",
    "        # print('打印用户问题', question)\n",
    "        if question == None or len(question) < 1:\n",
    "            print('问答为空。。。。')\n",
    "            return '', chat_history\n",
    "        # try:\n",
    "        print('调用检索问答链。。。。')\n",
    "        # 结果调用下流式输出\n",
    "        # response = self.qa_chain.invoke({'query': question})['result']\n",
    "        \n",
    "        # 检索问答链+历史聊天组件\n",
    "        response = self.qa_chain.invoke({'query': question, 'chat_history': chat_history})['answer']\n",
    "\n",
    "        chat_history.append([question, response])\n",
    "        # print(chat_history)\n",
    "        return '', chat_history\n",
    "                \n",
    "        # except Exception as e:\n",
    "        #     print('问答链报错', e)\n",
    "        #     return e, chat_history\n",
    "\n",
    "    def clear_history(self):\n",
    "        self.qa_chain.clear_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7abcb68b-dbaa-4139-941b-37014e7d7c2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化知识库问答链。。。。\n",
      "正从本地加载模型。。。。。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to \"AutoModelForCausalLM.from_pretrained\".\n",
      "Try importing flash-attention for faster inference...\n",
      "Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\n",
      "Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n",
      "Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb2808e17d4748dca95a68a9fd6a31b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "The repository for autodl-tmp/Qwen/Qwen-7B-Chat contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/autodl-tmp/Qwen/Qwen-7B-Chat.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n",
      "The repository for autodl-tmp/Qwen/Qwen-7B-Chat contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/autodl-tmp/Qwen/Qwen-7B-Chat.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型加载完成！\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ConversationalRetrievalChain\nchain_type_kwargs\n  extra fields not permitted (type=value_error.extra)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_center \u001b[38;5;241m=\u001b[39m \u001b[43mModel_center\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_chatbot\u001b[39m(question, chat_history):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m char \u001b[38;5;129;01min\u001b[39;00m model_center\u001b[38;5;241m.\u001b[39mqa_chain_self_answer(question, chat_history):\n",
      "Cell \u001b[0;32mIn[4], line 9\u001b[0m, in \u001b[0;36mModel_center.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, ):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m初始化知识库问答链。。。。\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqa_chain \u001b[38;5;241m=\u001b[39m \u001b[43mqa_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 46\u001b[0m, in \u001b[0;36mqa_chain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# 构造检索问答链\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# qa_chain_ = RetrievalQA.from_chain_type(\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m#     llm=llm,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m \n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# 构建历史对话检索问答\u001b[39;00m\n\u001b[1;32m     45\u001b[0m memory \u001b[38;5;241m=\u001b[39m ConversationBufferMemory(memory_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m'\u001b[39m, return_messages\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 46\u001b[0m qa_chain_ \u001b[38;5;241m=\u001b[39m \u001b[43mConversationalRetrievalChain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_llm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretriever\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvectordb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_retriever\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_source_documents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchain_type_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mQA_CHAIN_PROMPT\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \n\u001b[1;32m     53\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m qa_chain_\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/langchain/chains/conversational_retrieval/base.py:356\u001b[0m, in \u001b[0;36mConversationalRetrievalChain.from_llm\u001b[0;34m(cls, llm, retriever, condense_question_prompt, chain_type, verbose, condense_question_llm, combine_docs_chain_kwargs, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    349\u001b[0m _llm \u001b[38;5;241m=\u001b[39m condense_question_llm \u001b[38;5;129;01mor\u001b[39;00m llm\n\u001b[1;32m    350\u001b[0m condense_question_chain \u001b[38;5;241m=\u001b[39m LLMChain(\n\u001b[1;32m    351\u001b[0m     llm\u001b[38;5;241m=\u001b[39m_llm,\n\u001b[1;32m    352\u001b[0m     prompt\u001b[38;5;241m=\u001b[39mcondense_question_prompt,\n\u001b[1;32m    353\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    354\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m    355\u001b[0m )\n\u001b[0;32m--> 356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretriever\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretriever\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcombine_docs_chain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoc_chain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquestion_generator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcondense_question_chain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/langchain/load/serializable.py:75\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for ConversationalRetrievalChain\nchain_type_kwargs\n  extra fields not permitted (type=value_error.extra)"
     ]
    }
   ],
   "source": [
    "model_center = Model_center()\n",
    "def update_chatbot(question, chat_history):\n",
    "    for char in model_center.qa_chain_self_answer(question, chat_history):\n",
    "        gr.update(value=chat_history)\n",
    "        chat_history.append((question, char))\n",
    "    return chat_history\n",
    "# def demo():\n",
    "block = gr.Blocks()\n",
    "with block as demo:\n",
    "    with gr.Row(equal_height=True):  # 水平排列子组件\n",
    "        with gr.Column(scale=15):  # 垂直排列子组件\n",
    "            gr.Markdown(\"\"\"<h1><center>QwenLM7B-Chat</center></h1><center>科大讯飞实践-招中标政策智能问答助手</center>\"\"\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=4):\n",
    "            # 创建聊天界面的组件。height=450 参数设置了聊天界面的高度为 450 像素。\n",
    "            # show_copy_button=True参数表示在聊天界面中显示一个复制按钮，允许用户复制聊天内容\n",
    "            chatbot = gr.Chatbot(height=450, show_copy_button=True)\n",
    "            # 创建一个文本框组件，用于输入 prompts。\n",
    "            msg = gr.Textbox(label='Prompt/问题')\n",
    "\n",
    "            with gr.Row():\n",
    "                # 创建提交按钮\n",
    "                db_wo_his_btn = gr.Button('Chat')\n",
    "            with gr.Row():\n",
    "                # 创建一个清除按钮，用于清除聊天机器人组件的内容。\n",
    "                clear_btn = gr.ClearButton(components=[chatbot], value='Clear console')\n",
    "\n",
    "        # 设置按钮的点击事件。当点击时，调用上面定义的 qa_chain_self_answer 函数，并传入用户的消息和聊天历史记录，然后更新文本框和聊天机器人组件。\n",
    "        print('进度1')\n",
    "        # 设置流式输出\n",
    "        def bot(question, history):\n",
    "            # print('bot_question',question)\n",
    "            # print('bot_history',history)\n",
    "            curr, response = model_center.qa_chain_self_answer(question, history)\n",
    "            # print('response', response)\n",
    "            # print('curr', curr)\n",
    "            history = response\n",
    "            bot_message = history[-1][1]\n",
    "            # print('bot_message', bot_message)\n",
    "            history[-1][1] = ''\n",
    "            for character in bot_message:\n",
    "                history[-1][1] += character\n",
    "                # print(f'累计中：{history}')\n",
    "                time.sleep(0.1)\n",
    "                yield '', history\n",
    "        \n",
    "        db_wo_his_btn.click(bot, inputs=[msg, chatbot], outputs=[msg, chatbot])\n",
    "\n",
    "        print('进度2')\n",
    "        # 点击后清空后端存储的聊天记录\n",
    "        clear_btn.click(model_center.clear_history)\n",
    "\n",
    "    # 填写注意事项\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        提醒：<br>\n",
    "        1. 初始化数据库实践可能较长，请耐心等待。\n",
    "        2. 使用中如果出现异常，将会在文本输入框进行展示，请不要惊慌。 <br>\n",
    "        \"\"\"\n",
    "    )\n",
    "# gr.close_all()\n",
    "# 直接启动\n",
    "demo.queue()\n",
    "demo.launch(server_name='127.0.0.1', server_port=6005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa35931c-ac49-44fd-aa81-a3a782af46a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3966af8-c640-48ba-a724-96b0e6e0d008",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
